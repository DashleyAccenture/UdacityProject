{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "import os\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from lib.tooling import tool\n",
    "from dotenv import load_dotenv\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "##chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "##collection = chroma_client.get_collection(name=\"udaplay\", embedding_function=embedding_fn)\n",
    "\n",
    "##Retrieve_game tool\n",
    "def retrieve_game(query: str, n_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Semantic search: Finds most relevant results in the vector DB.\n",
    "\n",
    "    Args:\n",
    "        query: A question or description about games (platforms, names, years, etc.).\n",
    "        n_results: How many top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts, each containing:\n",
    "            - Platform\n",
    "            - Name\n",
    "            - YearOfRelease\n",
    "            - Description\n",
    "            - id (the Chroma document id)\n",
    "            - score (similarity score derived from distance; higher is better)\n",
    "    \"\"\"\n",
    "    if not query or not isinstance(query, str):\n",
    "        raise ValueError(\"`query` must be a non-empty string.\")\n",
    "\n",
    "    # Chroma query returns lists grouped by each input query_texts item.\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"metadatas\", \"documents\", \"ids\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Defensive extraction: Chroma returns nested lists, one per query.\n",
    "    metadatas = res.get(\"metadatas\", [[]])[0]\n",
    "    documents = res.get(\"documents\", [[]])[0]\n",
    "    ids       = res.get(\"ids\", [[]])[0]\n",
    "    distances = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(ids)):\n",
    "        meta = metadatas[i] if i < len(metadatas) else {}\n",
    "        doc  = documents[i] if i < len(documents) else \"\"\n",
    "        dist = distances[i] if i < len(distances) else None\n",
    "\n",
    "        # Convert Chroma distance to a 0..1 similarity score (heuristic).\n",
    "        # Chroma's \"distance\" is typically cosine distance (0 is identical).\n",
    "        # We map it to score = 1 - min(max(dist,0),1). If dist > 1, clamp to 0.\n",
    "        if dist is None:\n",
    "            score = None\n",
    "        else:\n",
    "            score = 1.0 - max(0.0, min(float(dist), 1.0))\n",
    "\n",
    "        # Normalize metadata keys expected from your add loop\n",
    "        results.append({\n",
    "            \"Platform\":     meta.get(\"Platform\"),\n",
    "            \"Name\":         meta.get(\"Name\"),\n",
    "            \"YearOfRelease\": meta.get(\"YearOfRelease\"),\n",
    "            \"Description\":  meta.get(\"Description\"),\n",
    "            \"id\":           ids[i],\n",
    "            \"score\":        score,\n",
    "            # Optional: include the raw document string\n",
    "            \"document\":     doc\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result\n",
    "\n",
    "##Evaluate_retrieval:\n",
    "\n",
    "# tools_evaluate.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Data class for the LLM judge outcome.\n",
    "    - useful: whether the documents are useful to answer the question\n",
    "    - description: detailed explanation supporting the decision\n",
    "    \"\"\"\n",
    "    useful: bool\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Initialize environment and client once\n",
    "load_dotenv('.env')\n",
    "_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "_OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\")  # optional (e.g., Azure/OpenAI proxy)\n",
    "_MODEL_NAME = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "if not _OPENAI_API_KEY:\n",
    "    # Fail fast with a clear message; Udacity runner will surface this\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is missing in .env\")\n",
    "\n",
    "_client = OpenAI(api_key=_OPENAI_API_KEY, base_url=_OPENAI_BASE_URL)\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    retrieved_docs: List[Dict[str, Any]],\n",
    "    max_docs: int = 8,\n",
    ") -> EvaluationReport:\n",
    "    \"\"\"\n",
    "    Tool: evaluate_retrieval\n",
    "    ------------------------\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    Args:\n",
    "        - question: original question from user\n",
    "        - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    Returns:\n",
    "        EvaluationReport:\n",
    "            - useful: whether the documents are useful to answer the question\n",
    "            - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        return EvaluationReport(useful=False, description=\"Invalid question.\")\n",
    "    if not retrieved_docs:\n",
    "        return EvaluationReport(useful=False, description=\"No documents retrieved to evaluate.\")\n",
    "\n",
    "    # Compact, LLM-friendly view of docs (limit to max_docs, truncate long descriptions)\n",
    "    lines: List[str] = []\n",
    "    for i, d in enumerate(retrieved_docs[:max_docs], start=1):\n",
    "        name = str(d.get(\"Name\", \"Unknown\"))\n",
    "        plat = str(d.get(\"Platform\", \"\"))\n",
    "        year = str(d.get(\"YearOfRelease\", \"\"))\n",
    "        desc = str(d.get(\"Description\", \"\"))[:500]  # keep prompt size reasonable\n",
    "        score = d.get(\"score\")\n",
    "        doc_id = d.get(\"id\")\n",
    "        lines.append(\n",
    "            f\"Doc {i}: Name={name}; Platform={plat}; Year={year}; Score={score}; Id={doc_id}; Description={desc}\"\n",
    "        )\n",
    "    docs_block = \"\\n\".join(lines)\n",
    "\n",
    "    # LLM judge prompt per TODO\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator.\n",
    "Your task is to evaluate if the provided documents are enough to respond to the query.\n",
    "\n",
    "Query:\n",
    "\\\"\\\"\\\"{question}\\\"\\\"\\\"\n",
    "\n",
    "Documents:\n",
    "{docs_block}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the documents, as a set, are sufficient and relevant to answer the query.\n",
    "- Consider coverage of key facts the query implies (e.g., developer, release date, platform), when applicable.\n",
    "- If not sufficient, explain what's missing or ambiguous.\n",
    "- Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond ONLY in strict JSON with the following keys:\n",
    "- \"useful\": true or false\n",
    "- \"description\": a concise but informative explanation\n",
    "\"\"\"\n",
    "\n",
    "    # Call the model; enforce JSON output if supported by your SDK version\n",
    "    try:\n",
    "        response = _client.responses.create(\n",
    "            model=_MODEL_NAME,\n",
    "            input=prompt,\n",
    "            response_format={\"type\": \"json_object\"},  # helps ensure valid JSON\n",
    "        )\n",
    "        raw_text = response.output_text.strip()\n",
    "    except Exception as e:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM evaluation failed: {e}\"\n",
    "        )\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(raw_text)\n",
    "        useful = bool(parsed.get(\"useful\"))\n",
    "        description = str(parsed.get(\"description\", \"\")).strip() or \"No description provided.\"\n",
    "        return EvaluationReport(useful=useful, description=description)\n",
    "    except Exception:\n",
    "        # Fallback if model returned non-JSON\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM response could not be parsed as JSON: {raw_text}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "query = \"in what year was need for speed first sold and by whom?\"\n",
    "\n",
    "##Game web Search - External search helper (graceful fallback)\n",
    "\n",
    "def game_web_search(query: str, max_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Lightweight web search helper for game-related queries.\n",
    "    Tries to fetch simple web results. Falls back gracefully if no internet.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (e.g., game name + platform + release year).\n",
    "        max_results: Max items to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts: { \"title\": str, \"url\": str, \"snippet\": str }.\n",
    "        If web access is unavailable, returns an empty list and a note in 'snippet'.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup  # requires 'beautifulsoup4' installed\n",
    "    except Exception:\n",
    "        # Fallback (no web libs)\n",
    "        return [{\n",
    "            \"title\": \"Web search unavailable\",\n",
    "            \"url\": \"\",\n",
    "            \"snippet\": \"Requests/BeautifulSoup not available in this environment.\"\n",
    "        }]\n",
    "\n",
    "    try:\n",
    "        # Very simple HTML search using DuckDuckGo (no API key)\n",
    "        resp = requests.get(\"https://duckduckgo.com/html/\", params={\"q\": query}, timeout=8)\n",
    "        if resp.status_code != 200:\n",
    "            return [{\n",
    "                \"title\": \"Web search failed\",\n",
    "                \"url\": \"\",\n",
    "                \"snippet\": f\"HTTP {resp.status_code} while searching for '{query}'.\"\n",
    "            }]\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        results = []\n",
    "        for a in soup.select(\".result__a\")[:max_results]:\n",
    "            title = a.get_text(strip=True)\n",
    "            url = a.get(\"href\", \"\")\n",
    "            snippet_tag = a.find_parent(\"div\", class_=\"result\").select_one(\".result__snippet\")\n",
    "            snippet = snippet_tag.get_text(strip=True) if snippet_tag else \"\"\n",
    "            results.append({\"title\": title, \"url\": url, \"snippet\": snippet})\n",
    "        if not results:\n",
    "            results.append({\"title\": \"No results parsed\", \"url\": \"\", \"snippet\": \"Parsing returned no items.\"})\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"title\": \"Web search error\",\n",
    "            \"url\": \"\",\n",
    "            \"snippet\": f\"Exception during web search: {e}\"\n",
    "        }]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_evaluate.py\n",
    "from tools_retrieve import retrieve_game\n",
    "from tools_evaluate import evaluate_retrieval\n",
    "\n",
    "query = 'Who developed \"FIFA 21\"?'\n",
    "results = retrieve_game(query, n_results=5)\n",
    "\n",
    "print(\"\\nRetrieved Results:\")\n",
    "for r in results:\n",
    "    print(f\"{r['Name']} | score={r['score']} | Year={r['YearOfRelease']}\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "metrics = evaluate_retrieval(query, results)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31c56281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2431.55s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Code/project/starter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2437.15s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t\t  __pycache__  games\t  test_evaluate_llm.py\n",
      "Udaplay_01_starter_project.ipynb  agent.py     lib\t  test_tools.py\n",
      "Udaplay_02_starter_project.ipynb  chromadb     models.py  tools.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed\n",
    "\n",
    "\n",
    "class AgentState(Enum):\n",
    "    ASK = auto()\n",
    "    RAG = auto()\n",
    "    EVAL = auto()\n",
    "    WEB = auto()\n",
    "    PARSE = auto()\n",
    "    STORE = auto()\n",
    "    REPORT = auto()\n",
    "\n",
    "class UdaPlayAgent:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def run(self, question: str) -> Dict[str, str]:\n",
    "        state = AgentState.ASK\n",
    "        rag_hits: List[RetrievalHit] = []\n",
    "        web_records: List[GameRecord] = []\n",
    "        confidence = 0.0\n",
    "        metrics = {}\n",
    "        resolved: Dict[str, str] = {}\n",
    "        reasoning_steps: List[str] = []\n",
    "\n",
    "        # RAG\n",
    "        state = AgentState.RAG\n",
    "        rag_hits = retrieve_game(question)\n",
    "        reasoning_steps.append(f\"RAG returned {len(rag_hits)} hits.\")\n",
    "\n",
    "        # Evaluate\n",
    "        state = AgentState.EVAL\n",
    "        confidence, metrics = evaluate_retrieval(question, rag_hits)\n",
    "        reasoning_steps.append(f\"Evaluation metrics: {metrics} → confidence={confidence:.3f}.\")\n",
    "\n",
    "        # Decide fallback\n",
    "        if confidence < MIN_CONFIDENCE:\n",
    "            state = AgentState.WEB\n",
    "            web_records = game_web_search(question)\n",
    "            reasoning_steps.append(f\"Fallback to web produced {len(web_records)} candidates.\")\n",
    "            state = AgentState.PARSE\n",
    "            # prefer first web record; refine resolved fields\n",
    "            best_web = web_records[0] if web_records else None\n",
    "            if best_web:\n",
    "                resolved = {\n",
    "                    \"title\": best_web.title or _infer_title(question),\n",
    "                    \"developer\": best_web.developer or \"\",\n",
    "                    \"publisher\": best_web.publisher or \"\",\n",
    "                    \"release_date\": best_web.release_date or \"\",\n",
    "                    \"platforms\": \", \".join(best_web.platforms) if best_web.platforms else \"\"\n",
    "                }\n",
    "                # store memory\n",
    "                state = AgentState.STORE\n",
    "                added = persist_new_knowledge(web_records[:3])\n",
    "                reasoning_steps.append(f\"Persisted {added} new web‑sourced records.\")\n",
    "                # recompute confidence (boost slightly due to fresh authoritative source)\n",
    "                confidence = min(1.0, max(confidence, 0.72))\n",
    "        else:\n",
    "            # Resolve from local hits (majority vote on top 3)\n",
    "            resolved = _resolve_from_local(question, rag_hits[:3])\n",
    "            reasoning_steps.append(\"Resolved facts from local dataset.\")\n",
    "\n",
    "        state = AgentState.REPORT\n",
    "        from report import render\n",
    "        report = build_report(\n",
    "            question=question,\n",
    "            resolved=resolved,\n",
    "            confidence=confidence,\n",
    "            sources_local=rag_hits,\n",
    "            sources_web=web_records,\n",
    "            reasoning=\" → \".join(reasoning_steps)\n",
    "        )\n",
    "        return {\"markdown\": render(report)}\n",
    "\n",
    "def _infer_title(question: str) -> str:\n",
    "    import re\n",
    "    m = re.search(r\"“([^”]+)”|\\\"([^\\\"]+)\\\"\", question)\n",
    "    return m.group(1) or m.group(2) if m else \"\"\n",
    "\n",
    "def _resolve_from_local(question: str, hits: List[RetrievalHit]) -> Dict[str, str]:\n",
    "    # choose best hit and pull structured fields (simple heuristic: highest score)\n",
    "    if not hits: return {}\n",
    "    best = sorted(hits, key=lambda h: h.score, reverse=True)[0]\n",
    "    resolved = {\n",
    "        \"title\": best.title,\n",
    "        \"developer\": best.record.developer or \"\",\n",
    "        \"publisher\": best.record.publisher or \"\",\n",
    "        \"release_date\": best.record.release_date or \"\",\n",
    "        \"platforms\": \", \".join(best.record.platforms) if best.record.platforms else \"\"\n",
    "    }\n",
    "    return resolved\n",
    "\n",
    "    \n",
    "!pwd\n",
    "!ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: Invoke your agent\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# - When Pokémon Gold and Silver was released?\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# - Which one was the first 3D platformer Mario game?\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# - Which one was the first 3D platformer Mario game?\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# - Was Mortal Kombat X released for Playstation 5?\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UdaPlayAgent  \u001b[38;5;66;03m# Ensure agent.py contains the UdaPlayAgent class\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Instantiate the agent\u001b[39;00m\n\u001b[32m     15\u001b[39m agent = UdaPlayAgent()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/project/starter/agent.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalHit, GameRecord\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retrieve_game, evaluate_retrieval, game_web_search\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_report\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m persist_new_knowledge\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/project/starter/tools.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtavily\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TavilyClient\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalHit, GameRecord\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m semantic_search\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvector_store\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreManager\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MIN_CONFIDENCE, TAVILY_MAX_RESULTS, TAVILY_INCLUDE_RAW, COVERAGE_WEIGHT, RELEVANCE_WEIGHT, CONSENSUS_WEIGHT\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'search'"
     ]
    }
   ],
   "source": [
    "# TODO: Invoke your agent\n",
    "# - When Pokémon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X realeased for Playstation 5?\n",
    "\n",
    "# TODO: Invoke your agent\n",
    "# - When Pokémon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X released for Playstation 5?\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from agent import UdaPlayAgent  # Ensure agent.py contains the UdaPlayAgent class\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = UdaPlayAgent()\n",
    "\n",
    "# Questions to ask\n",
    "questions = [\n",
    "    \"When Pokémon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "]\n",
    "\n",
    "print(\"### UdaPlay Agent Responses ###\\n\")\n",
    "for q in questions:\n",
    "    result = agent.run(q)  # agent.run returns {\"markdown\": render(report)}\n",
    "    display(Markdown(result[\"markdown\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0 (main, Dec  3 2024, 02:24:14) [GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
