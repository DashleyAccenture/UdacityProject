{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "# # Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6eac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read current content of workspace memory path into memory\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Read current content\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Removing old version of persist_new_knowledge...\n",
      "‚úÖ Adding corrected persist_new_knowledge function...\n",
      "‚úÖ Function added! Restart kernel and try again.\n"
     ]
    }
   ],
   "source": [
    "# Fix: Add or update persist_new_knowledge function in workspace memory.py\n",
    "import os\n",
    "\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Read current content\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Remove old version if it exists\n",
    "if 'def persist_new_knowledge' in content:\n",
    "    print(\"‚ö†Ô∏è Removing old version of persist_new_knowledge...\")\n",
    "    # Find and remove the old function\n",
    "    lines = content.split('\\n')\n",
    "    new_lines = []\n",
    "    skip = False\n",
    "    for line in lines:\n",
    "        if 'def persist_new_knowledge' in line:\n",
    "            skip = True\n",
    "        elif skip and line and not line[0].isspace() and line.strip():\n",
    "            skip = False\n",
    "        \n",
    "        if not skip:\n",
    "            new_lines.append(line)\n",
    "    \n",
    "    content = '\\n'.join(new_lines)\n",
    "    with open(workspace_memory_path, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Add the corrected function\n",
    "print(\"‚úÖ Adding corrected persist_new_knowledge function...\")\n",
    "with open(workspace_memory_path, 'a') as f:\n",
    "    f.write('''\n",
    "\n",
    "# --- Persist new knowledge ---\n",
    "def persist_new_knowledge(records):\n",
    "    \"\"\"\n",
    "    Store web-sourced game records into the vector store for future retrieval.\n",
    "    \n",
    "    Args:\n",
    "        records: List of dict objects with game info to persist\n",
    "    \n",
    "    Returns:\n",
    "        Number of records successfully added\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    from lib.vector_store import VectorStoreManager\n",
    "    vsm = VectorStoreManager()\n",
    "    added = 0\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            # Handle both dict and object types\n",
    "            if isinstance(record, dict):\n",
    "                title = record.get('title', '')\n",
    "                snippet = record.get('snippet', '')\n",
    "                developer = record.get('developer', '')\n",
    "                platforms = record.get('platforms', '')\n",
    "                url = record.get('url', '')\n",
    "            else:\n",
    "                title = getattr(record, 'title', '')\n",
    "                snippet = getattr(record, 'snippet', '')\n",
    "                developer = getattr(record, 'developer', '')\n",
    "                platforms = getattr(record, 'platforms', '')\n",
    "                url = getattr(record, 'url', '')\n",
    "            \n",
    "            # Build document text for embedding\n",
    "            doc_text = f\"{title}\"\n",
    "            if snippet:\n",
    "                doc_text += f\" - {snippet}\"\n",
    "            if developer:\n",
    "                doc_text += f\" | Developer: {developer}\"\n",
    "            if platforms:\n",
    "                doc_text += f\" | Platforms: {platforms}\"\n",
    "            \n",
    "            # Build metadata\n",
    "            metadata = {\n",
    "                \"title\": title,\n",
    "                \"developer\": developer or \"\",\n",
    "                \"platforms\": platforms or \"\",\n",
    "                \"source\": \"web\",\n",
    "                \"url\": url or \"\",\n",
    "            }\n",
    "            \n",
    "            # Add to vector store\n",
    "            import hashlib\n",
    "            doc_id = hashlib.md5(title.encode()).hexdigest()\n",
    "            vsm.add_document(doc_id=doc_id, text=doc_text, metadata=metadata)\n",
    "            added += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to persist record: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return added\n",
    "''')\n",
    "\n",
    "print(\"‚úÖ Function added! Restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7613211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ persist_new_knowledge function exists\n",
      "Function signature: (records)\n"
     ]
    }
   ],
   "source": [
    "# Check if persist_new_knowledge exists in workspace memory.py\n",
    "import memory\n",
    "import inspect\n",
    "\n",
    "if not hasattr(memory, 'persist_new_knowledge'):\n",
    "    print(\"‚ö†Ô∏è persist_new_knowledge function is missing from memory.py\")\n",
    "    print(f\"Memory module location: {memory.__file__}\")\n",
    "    print(\"\\nYou need to add this function to your workspace memory.py file.\")\n",
    "else:\n",
    "    print(\"‚úÖ persist_new_knowledge function exists\")\n",
    "    print(f\"Function signature: {inspect.signature(memory.persist_new_knowledge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce74db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# # Connect to the persistent ChromaDB created in Part 1\n",
    "# chroma_client = chromadb(path=\"chromadb\")\n",
    "\n",
    "# # Get the embedding function (must match Part 1)\n",
    "# embedding_fn = OpenAIEmbeddingFunction(\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "# # Get the existing collection\n",
    "# collection = chroma_client.get_collection(\n",
    "#     name=\"udaplay\", \n",
    "#     embedding_function=embedding_fn\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ Connected to collection: {collection.name}\")\n",
    "# print(f\"   Documents in collection: {collection.count()}\")\n",
    "\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# # Connect to the persistent ChromaDB created in Part 1\n",
    "# chroma_client = chromadb.PersistentClient(path=\"chromadb\")  # ‚Üê Fixed: was missing .PersistentClient\n",
    "\n",
    "# # Get the embedding function (must match Part 1)\n",
    "# embedding_fn = OpenAIEmbeddingFunction(\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "# # Get the existing collection\n",
    "# collection = chroma_client.get_collection(\n",
    "#     name=\"udaplay\", \n",
    "#     embedding_function=embedding_fn\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ Connected to collection: {collection.name}\")\n",
    "# print(f\"   Documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac549f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "import os\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from lib.tooling import tool\n",
    "from dotenv import load_dotenv\n",
    "# from enum import Enum, auto\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "tavily_key = TAVILY_API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80a7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to collection: udaplay\n",
      "   Documents in collection: 15\n"
     ]
    }
   ],
   "source": [
    "## added this code here toreconnect to embeddings from part 1 and proceed in the correct order.\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# Connect to the persistent ChromaDB created in Part 1\n",
    "chroma_client = chromadb.PersistentClient(path=\"chromadb\")  # ‚Üê Fixed: was missing .PersistentClient\n",
    "\n",
    "# Get the embedding function (must match Part 1)\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Get the existing collection\n",
    "collection = chroma_client.get_collection(\n",
    "    name=\"udaplay\", \n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Connected to collection: {collection.name}\")\n",
    "print(f\"   Documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "##chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "##collection = chroma_client.get_collection(name=\"udaplay\", embedding_function=embedding_fn)\n",
    "\n",
    "##Retrieve_game tool\n",
    "def retrieve_game(query: str, n_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Semantic search: Finds most relevant results in the vector DB.\n",
    "\n",
    "    Args:\n",
    "        query: A question or description about games (platforms, names, years, etc.).\n",
    "        n_results: How many top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts, each containing:\n",
    "            - Platform\n",
    "            - Name\n",
    "            - YearOfRelease\n",
    "            - Description\n",
    "            - id (the Chroma document id)\n",
    "            - score (similarity score derived from distance; higher is better)\n",
    "    \"\"\"\n",
    "    if not query or not isinstance(query, str):\n",
    "        raise ValueError(\"`query` must be a non-empty string.\")\n",
    "\n",
    "    # Chroma query returns lists grouped by each input query_texts item.\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "         include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Defensive extraction: Chroma returns nested lists, one per query.\n",
    "    metadatas = res.get(\"metadatas\", [[]])[0]\n",
    "    documents = res.get(\"documents\", [[]])[0]\n",
    "    ids       = res.get(\"ids\", [[]])[0]\n",
    "    distances = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(ids)):\n",
    "        meta = metadatas[i] if i < len(metadatas) else {}\n",
    "        doc  = documents[i] if i < len(documents) else \"\"\n",
    "        dist = distances[i] if i < len(distances) else None\n",
    "\n",
    "        # Convert Chroma distance to a 0..1 similarity score (heuristic).\n",
    "        # Chroma's \"distance\" is typically cosine distance (0 is identical).\n",
    "        # We map it to score = 1 - min(max(dist,0),1). If dist > 1, clamp to 0.\n",
    "        if dist is None:\n",
    "            score = None\n",
    "        else:\n",
    "            score = 1.0 - max(0.0, min(float(dist), 1.0))\n",
    "\n",
    "        # Normalize metadata keys expected from your add loop\n",
    "        results.append({\n",
    "            \"Platform\":     meta.get(\"Platform\"),\n",
    "            \"Name\":         meta.get(\"Name\"),\n",
    "            \"YearOfRelease\": meta.get(\"YearOfRelease\"),\n",
    "            \"Description\":  meta.get(\"Description\"),\n",
    "            \"id\":           ids[i],\n",
    "            \"score\":        score,\n",
    "            # Optional: include the raw document string\n",
    "            \"document\":     doc\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result\n",
    "\n",
    "##Evaluate_retrieval:\n",
    "\n",
    "# tools_evaluate.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Data class for the LLM judge outcome.\n",
    "    - useful: whether the documents are useful to answer the question\n",
    "    - description: detailed explanation supporting the decision\n",
    "    \"\"\"\n",
    "    useful: bool\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Initialize environment and client once\n",
    "load_dotenv('.env')\n",
    "_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "_OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\")  # optional (e.g., Azure/OpenAI proxy)\n",
    "_MODEL_NAME = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "if not _OPENAI_API_KEY:\n",
    "    # Fail fast with a clear message; Udacity runner will surface this\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is missing in .env\")\n",
    "\n",
    "_client = OpenAI(api_key=_OPENAI_API_KEY, base_url=_OPENAI_BASE_URL)\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    retrieved_docs: List[Dict[str, Any]],\n",
    "    max_docs: int = 8,\n",
    ") -> EvaluationReport:\n",
    "    \"\"\"\n",
    "    Tool: evaluate_retrieval\n",
    "    ------------------------\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    Args:\n",
    "        - question: original question from user\n",
    "        - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    Returns:\n",
    "        EvaluationReport:\n",
    "            - useful: whether the documents are useful to answer the question\n",
    "            - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        return EvaluationReport(useful=False, description=\"Invalid question.\")\n",
    "    if not retrieved_docs:\n",
    "        return EvaluationReport(useful=False, description=\"No documents retrieved to evaluate.\")\n",
    "\n",
    "    # Compact, LLM-friendly view of docs (limit to max_docs, truncate long descriptions)\n",
    "    lines: List[str] = []\n",
    "    for i, d in enumerate(retrieved_docs[:max_docs], start=1):\n",
    "        name = str(d.get(\"Name\", \"Unknown\"))\n",
    "        plat = str(d.get(\"Platform\", \"\"))\n",
    "        year = str(d.get(\"YearOfRelease\", \"\"))\n",
    "        desc = str(d.get(\"Description\", \"\"))[:500]  # keep prompt size reasonable\n",
    "        score = d.get(\"score\")\n",
    "        doc_id = d.get(\"id\")\n",
    "        lines.append(\n",
    "            f\"Doc {i}: Name={name}; Platform={plat}; Year={year}; Score={score}; Id={doc_id}; Description={desc}\"\n",
    "        )\n",
    "    docs_block = \"\\n\".join(lines)\n",
    "\n",
    "    # LLM judge prompt per TODO\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator.\n",
    "Your task is to evaluate if the provided documents are enough to respond to the query.\n",
    "\n",
    "Query:\n",
    "\\\"\\\"\\\"{question}\\\"\\\"\\\"\n",
    "\n",
    "Documents:\n",
    "{docs_block}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the documents, as a set, are sufficient and relevant to answer the query.\n",
    "- Consider coverage of key facts the query implies (e.g., developer, release date, platform), when applicable.\n",
    "- If not sufficient, explain what's missing or ambiguous.\n",
    "- Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond ONLY in strict JSON with the following keys:\n",
    "- \"useful\": true or false\n",
    "- \"description\": a concise but informative explanation\n",
    "\"\"\"\n",
    "\n",
    "   \n",
    "    # Call the model; enforce JSON output if supported by your SDK version\n",
    "    try:\n",
    "        response = _client.chat.completions.create(\n",
    "            model=_MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},  # helps ensure valid JSON\n",
    "        )\n",
    "        raw_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Call the model; enforce JSON output if supported by your SDK version    \n",
    "    # try:\n",
    "    #     response = _client.responses.create(\n",
    "    #         model=_MODEL_NAME,\n",
    "    #         input=prompt,\n",
    "    #         response_format={\"type\": \"json_object\"},  # helps ensure valid JSON\n",
    "    #     )\n",
    "    #     raw_text = response.output_text.strip()\n",
    "    except Exception as e:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM evaluation failed: {e}\"\n",
    "        )\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(raw_text)\n",
    "        useful = bool(parsed.get(\"useful\"))\n",
    "        description = str(parsed.get(\"description\", \"\")).strip() or \"No description provided.\"\n",
    "        return EvaluationReport(useful=useful, description=description)\n",
    "    except Exception:\n",
    "        # Fallback if model returned non-JSON\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM response could not be parsed as JSON: {raw_text}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "query = \"in what year was need for speed first sold and by whom?\"\n",
    "\n",
    "##Game web Search - External search helper (graceful fallback)\n",
    "\n",
    "# def game_web_search(query: str, max_results: int = 5) -> list[dict]:\n",
    "#     \"\"\"\n",
    "#     Lightweight web search helper for game-related queries.\n",
    "#     Tries to fetch simple web results. Falls back gracefully if no internet.\n",
    "\n",
    "#     Args:\n",
    "#         query: Search query (e.g., game name + platform + release year).\n",
    "#         max_results: Max items to return.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of dicts: { \"title\": str, \"url\": str, \"snippet\": str }.\n",
    "#         If web access is unavailable, returns an empty list and a note in 'snippet'.\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "query = \"in what year was need for speed first sold and by whom?\"\n",
    "\n",
    "##Game web Search - External search helper (graceful fallback)\n",
    "\n",
    "def game_web_search(query: str, max_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Lightweight web search helper for game-related queries.\n",
    "    Uses Tavily for web search.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (e.g., game name + platform + release year).\n",
    "        max_results: Max items to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts: { \"title\": str, \"url\": str, \"snippet\": str, \"developer\": str, \"publisher\": str, \"release_date\": str, \"platforms\": str }.\n",
    "    \"\"\"\n",
    "    from tavily import TavilyClient\n",
    "\n",
    "    try:\n",
    "        client = TavilyClient(api_key=tavily_key)\n",
    "        response = client.search(query, max_results=max_results)\n",
    "        results = []\n",
    "        for r in response['results']:\n",
    "            results.append({\n",
    "                \"title\": r['title'],\n",
    "                \"url\": r['url'],\n",
    "                \"snippet\": r['content'],\n",
    "                \"developer\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"release_date\": \"\",\n",
    "                \"platforms\": \"\"\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"title\": \"Web search error\",\n",
    "            \"url\": \"\",\n",
    "            \"snippet\": f\"Exception during web search: {e}\",\n",
    "            \"developer\": \"\",\n",
    "            \"publisher\": \"\",\n",
    "            \"release_date\": \"\",\n",
    "            \"platforms\": \"\"\n",
    "        }]\n",
    "\n",
    "# Eval retrieval tool\n",
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    retrieved_docs: List[Dict[str, Any]],\n",
    "    max_docs: int = 8,\n",
    ") -> EvaluationReport:\n",
    "    \"\"\"\n",
    "    Tool: evaluate_retrieval\n",
    "    ------------------------\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    Args:\n",
    "        - question: original question from user\n",
    "        - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    Returns:\n",
    "        EvaluationReport:\n",
    "            - useful: whether the documents are useful to answer the question\n",
    "            - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        return EvaluationReport(useful=False, description=\"Invalid question.\")\n",
    "    if not retrieved_docs:\n",
    "        return EvaluationReport(useful=False, description=\"No documents retrieved to evaluate.\")\n",
    "\n",
    "    # Compact, LLM-friendly view of docs\n",
    "    lines: List[str] = []\n",
    "    for i, d in enumerate(retrieved_docs[:max_docs], start=1):\n",
    "        name = str(d.get(\"Name\", \"Unknown\"))\n",
    "        plat = str(d.get(\"Platform\", \"\"))\n",
    "        year = str(d.get(\"YearOfRelease\", \"\"))\n",
    "        desc = str(d.get(\"Description\", \"\"))[:500]\n",
    "        score = d.get(\"score\")\n",
    "        doc_id = d.get(\"id\")\n",
    "        lines.append(\n",
    "            f\"Doc {i}: Name={name}; Platform={plat}; Year={year}; Score={score}; Id={doc_id}; Description={desc}\"\n",
    "        )\n",
    "    docs_block = \"\\n\".join(lines)\n",
    "\n",
    "    # LLM judge prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator.\n",
    "Your task is to evaluate if the provided documents are enough to respond to the query.\n",
    "\n",
    "Query:\n",
    "\\\"\\\"\\\"{question}\\\"\\\"\\\"\n",
    "\n",
    "Documents:\n",
    "{docs_block}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the documents, as a set, are sufficient and relevant to answer the query.\n",
    "- Consider coverage of key facts the query implies (e.g., developer, release date, platform), when applicable.\n",
    "- If not sufficient, explain what's missing or ambiguous.\n",
    "- Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond ONLY in strict JSON with the following keys:\n",
    "- \"useful\": true or false\n",
    "- \"description\": a concise but informative explanation\n",
    "\"\"\"\n",
    "\n",
    "    # Call the model\n",
    "    try:\n",
    "        response = _client.responses.create(\n",
    "            model=_MODEL_NAME,\n",
    "            input=prompt,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        raw_text = response.output_text.strip()\n",
    "    except Exception as e:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM evaluation failed: {e}\"\n",
    "        )\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(raw_text)\n",
    "        useful = bool(parsed.get(\"useful\"))\n",
    "        description = str(parsed.get(\"description\", \"\")).strip() or \"No description provided.\"\n",
    "        return EvaluationReport(useful=useful, description=description)\n",
    "    except Exception:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM response could not be parsed as JSON: {raw_text}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game\n",
    "\n",
    "  \n",
    "  ## beautiful soup implementation.\n",
    "\n",
    "  \n",
    "  \n",
    "    # try:\n",
    "    #     import requests\n",
    "    #     # from bs4 import BeautifulSoup  # requires 'beautifulsoup4' installed\n",
    "    # except Exception:\n",
    "    #     # Fallback (no web libs)\n",
    "    #     return [{\n",
    "    #         \"title\": \"Web search unavailable\",\n",
    "    #         \"url\": \"\",\n",
    "    #         \"snippet\": \"Requests/BeautifulSoup not available in this environment.\"\n",
    "    #     }]\n",
    "\n",
    "#     try:\n",
    "#         # Very simple HTML search using DuckDuckGo (no API key)\n",
    "#         resp = requests.get(\"https://duckduckgo.com/html/\", params={\"q\": query}, timeout=8)\n",
    "#         if resp.status_code != 200:\n",
    "#             return [{\n",
    "#                 \"title\": \"Web search failed\",\n",
    "#                 \"url\": \"\",\n",
    "#                 \"snippet\": f\"HTTP {resp.status_code} while searching for '{query}'.\"\n",
    "#             }]\n",
    "#         soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#         results = []\n",
    "#         for a in soup.select(\".result__a\")[:max_results]:\n",
    "#             title = a.get_text(strip=True)\n",
    "#             url = a.get(\"href\", \"\")\n",
    "#             snippet_tag = a.find_parent(\"div\", class_=\"result\").select_one(\".result__snippet\")\n",
    "#             snippet = snippet_tag.get_text(strip=True) if snippet_tag else \"\"\n",
    "#             results.append({\"title\": title, \"url\": url, \"snippet\": snippet})\n",
    "#         if not results:\n",
    "#             results.append({\"title\": \"No results parsed\", \"url\": \"\", \"snippet\": \"Parsing returned no items.\"})\n",
    "#         return results\n",
    "#     except Exception as e:\n",
    "#         return [{\n",
    "#             \"title\": \"Web search error\",\n",
    "#             \"url\": \"\",\n",
    "#             \"snippet\": f\"Exception during web search: {e}\"\n",
    "#         }]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Tool Docstring:\n",
    "# #    Semantic search: Finds most results in the vector DB\n",
    "# #    args:\n",
    "# #    - query: a question about game industry. \n",
    "# #\n",
    "# #    You'll receive results as list. Each element contains:\n",
    "# #    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "# #    - Name: Name of the Game\n",
    "# #    - YearOfRelease: Year when that game was released for that platform\n",
    "# #    - Description: Additional details about the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # test_evaluate.py\n",
    "# from tools_retrieve import retrieve_game\n",
    "# # from tools_evaluate import evaluate_retrieval\n",
    "\n",
    "# query = 'Who developed \"FIFA 21\"?'\n",
    "# results = retrieve_game(query, n_results=5)\n",
    "\n",
    "# print(\"\\nRetrieved Results:\")\n",
    "# for r in results:\n",
    "#     print(f\"{r['Name']} | score={r['score']} | Year={r['YearOfRelease']}\")\n",
    "\n",
    "# print(\"\\nEvaluation Metrics:\")\n",
    "# metrics = evaluate_retrieval(query, results)\n",
    "# for k, v in metrics.items():\n",
    "#     print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List\n",
    "\n",
    "class AgentState(Enum):\n",
    "    ASK = auto()\n",
    "    RAG = auto()\n",
    "    EVAL = auto()\n",
    "    WEB = auto()\n",
    "    REPORT = auto()\n",
    "\n",
    "class UdaPlayAgent:\n",
    "    def __init__(self):\n",
    "        self.min_confidence_threshold = 0.6\n",
    "    \n",
    "    def run(self, question: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Run the agent through its state machine to answer a game industry question.\n",
    "        \n",
    "        States:\n",
    "        1. ASK - Receive question\n",
    "        2. RAG - Retrieve from vector DB\n",
    "        3. EVAL - Evaluate if RAG results are sufficient\n",
    "        4. WEB - If needed, search web\n",
    "        5. REPORT - Generate final answer\n",
    "        \"\"\"\n",
    "        state = AgentState.ASK\n",
    "        reasoning_steps = []\n",
    "        \n",
    "        # State: RAG - Retrieve from local vector DB\n",
    "        state = AgentState.RAG\n",
    "        rag_results = retrieve_game(question, n_results=5)\n",
    "        reasoning_steps.append(f\"Retrieved {len(rag_results)} documents from vector DB\")\n",
    "        \n",
    "        # State: EVAL - Evaluate retrieval quality\n",
    "        state = AgentState.EVAL\n",
    "        eval_report = evaluate_retrieval(question, rag_results)\n",
    "        reasoning_steps.append(f\"Evaluation: {eval_report.description}\")\n",
    "        \n",
    "        # State: WEB - Search web if RAG not sufficient\n",
    "        web_results = []\n",
    "        if not eval_report.useful:\n",
    "            state = AgentState.WEB\n",
    "            web_results = game_web_search(question, max_results=3)\n",
    "            reasoning_steps.append(f\"Used web search, found {len(web_results)} results\")\n",
    "        \n",
    "        # State: REPORT - Generate markdown report\n",
    "        state = AgentState.REPORT\n",
    "        markdown_report = self._build_markdown_report(\n",
    "            question=question,\n",
    "            rag_results=rag_results,\n",
    "            web_results=web_results,\n",
    "            eval_report=eval_report,\n",
    "            reasoning=reasoning_steps\n",
    "        )\n",
    "        \n",
    "        return {\"markdown\": markdown_report}\n",
    "    \n",
    "    def _build_markdown_report(\n",
    "        self, \n",
    "        question: str, \n",
    "        rag_results: List[Dict], \n",
    "        web_results: List[Dict],\n",
    "        eval_report,\n",
    "        reasoning: List[str]\n",
    "    ) -> str:\n",
    "        \"\"\"Build a markdown report from agent results.\"\"\"\n",
    "        \n",
    "        # Header\n",
    "        report = f\"# üéÆ UdaPlay Agent Response\\n\\n\"\n",
    "        report += f\"**Question:** {question}\\n\\n\"\n",
    "        report += f\"**Confidence:** {'High' if eval_report.useful else 'Low (used web search)'}\\n\\n\"\n",
    "        \n",
    "        # Answer section\n",
    "        report += \"## Answer\\n\\n\"\n",
    "        \n",
    "        if eval_report.useful and rag_results:\n",
    "            # Answer from RAG\n",
    "            best_match = rag_results[0]\n",
    "            report += f\"**Game:** {best_match['Name']}\\n\\n\"\n",
    "            report += f\"**Platform:** {best_match.get('Platform', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Year:** {best_match.get('YearOfRelease', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Description:** {best_match.get('Description', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Relevance Score:** {best_match.get('score', 0):.3f}\\n\\n\"\n",
    "        elif web_results:\n",
    "            # Answer from web\n",
    "            for i, result in enumerate(web_results[:3], 1):\n",
    "                report += f\"### Source {i}: {result['title']}\\n\\n\"\n",
    "                report += f\"{result['snippet']}\\n\\n\"\n",
    "                report += f\"[Read more]({result['url']})\\n\\n\"\n",
    "        else:\n",
    "            report += \"No sufficient information found to answer this question.\\n\\n\"\n",
    "        \n",
    "        # Sources section\n",
    "        report += \"## üìö Sources\\n\\n\"\n",
    "        \n",
    "        if eval_report.useful:\n",
    "            report += \"### Vector Database Results\\n\\n\"\n",
    "            for i, doc in enumerate(rag_results[:3], 1):\n",
    "                report += f\"{i}. **{doc['Name']}** ({doc.get('YearOfRelease', 'N/A')}) - \"\n",
    "                report += f\"Score: {doc.get('score', 0):.3f}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        if web_results:\n",
    "            report += \"### Web Search Results\\n\\n\"\n",
    "            for i, result in enumerate(web_results[:3], 1):\n",
    "                report += f\"{i}. [{result['title']}]({result['url']})\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Reasoning section\n",
    "        report += \"## üîç Agent Reasoning\\n\\n\"\n",
    "        for i, step in enumerate(reasoning, 1):\n",
    "            report += f\"{i}. {step}\\n\"\n",
    "        \n",
    "        return report\n",
    "        \n",
    "        # # TODO: Create your Agent abstraction using StateMachine\n",
    "# # Equip with an appropriate model\n",
    "# # Craft a good set of instructions \n",
    "# # Plug all Tools you developed\n",
    "# from enum import Enum, auto\n",
    "# from typing import Dict, List\n",
    "\n",
    "# class AgentState(Enum):\n",
    "#     ASK = auto()\n",
    "#     RAG = auto()\n",
    "#     EVAL = auto()\n",
    "#     WEB = auto()\n",
    "#     PARSE = auto()\n",
    "#     STORE = auto()\n",
    "#     REPORT = auto()\n",
    "\n",
    "# class UdaPlayAgent:\n",
    "#     def __init__(self): pass\n",
    "#        self.min_confidence_threshold = 0.6\n",
    "\n",
    "#     def run(self, question: str) -> Dict[str, str]:\n",
    "#         state = AgentState.ASK\n",
    "#         rag_hits: List[RetrievalHit] = []\n",
    "#         web_records: List[GameRecord] = []\n",
    "#         confidence = 0.0\n",
    "#         metrics = {}\n",
    "#         resolved: Dict[str, str] = {}\n",
    "#         reasoning_steps: List[str] = []\n",
    "\n",
    "#         # RAG\n",
    "#         state = AgentState.RAG\n",
    "#         rag_hits = retrieve_game(question)\n",
    "#         reasoning_steps.append(f\"RAG returned {len(rag_hits)} hits.\")\n",
    "\n",
    "#         # Evaluate\n",
    "#         state = AgentState.EVAL\n",
    "#         confidence, metrics = evaluate_retrieval(question, rag_hits)\n",
    "#         reasoning_steps.append(f\"Evaluation metrics: {metrics} ‚Üí confidence={confidence:.3f}.\")\n",
    "\n",
    "#         # Decide fallback\n",
    "#         if confidence < MIN_CONFIDENCE:\n",
    "#             state = AgentState.WEB\n",
    "#             web_records = game_web_search(question)\n",
    "#             reasoning_steps.append(f\"Fallback to web produced {len(web_records)} candidates.\")\n",
    "#             state = AgentState.PARSE\n",
    "#             # prefer first web record; refine resolved fields\n",
    "#             best_web = web_records[0] if web_records else None\n",
    "#             if best_web:\n",
    "#                 resolved = {\n",
    "#                     \"title\": best_web.title or _infer_title(question),\n",
    "#                     \"developer\": best_web.developer or \"\",\n",
    "#                     \"publisher\": best_web.publisher or \"\",\n",
    "#                     \"release_date\": best_web.release_date or \"\",\n",
    "#                     \"platforms\": \", \".join(best_web.platforms) if best_web.platforms else \"\"\n",
    "#                 }\n",
    "#                 # store memory\n",
    "#                 state = AgentState.STORE\n",
    "#                 added = persist_new_knowledge(web_records[:3])\n",
    "#                 reasoning_steps.append(f\"Persisted {added} new web‚Äësourced records.\")\n",
    "#                 # recompute confidence (boost slightly due to fresh authoritative source)\n",
    "#                 confidence = min(1.0, max(confidence, 0.72))\n",
    "#         else:\n",
    "#             # Resolve from local hits (majority vote on top 3)\n",
    "#             resolved = _resolve_from_local(question, rag_hits[:3])\n",
    "#             reasoning_steps.append(\"Resolved facts from local dataset.\")\n",
    "\n",
    "#         state = AgentState.REPORT\n",
    "#         from report import render\n",
    "#         report = build_report(\n",
    "#             question=question,\n",
    "#             resolved=resolved,\n",
    "#             confidence=confidence,\n",
    "#             sources_local=rag_hits,\n",
    "#             sources_web=web_records,\n",
    "#             reasoning=\" ‚Üí \".join(reasoning_steps)\n",
    "#         )\n",
    "#         return {\"markdown\": render(report)}\n",
    "\n",
    "# def _infer_title(question: str) -> str:\n",
    "#     import re\n",
    "#     m = re.search(r\"‚Äú([^‚Äù]+)‚Äù|\\\"([^\\\"]+)\\\"\", question)\n",
    "#     return m.group(1) or m.group(2) if m else \"\"\n",
    "\n",
    "# def _resolve_from_local(question: str, hits: List[RetrievalHit]) -> Dict[str, str]:\n",
    "#     # choose best hit and pull structured fields (simple heuristic: highest score)\n",
    "#     if not hits: return {}\n",
    "#     best = sorted(hits, key=lambda h: h.score, reverse=True)[0]\n",
    "#     resolved = {\n",
    "#         \"title\": best.title,\n",
    "#         \"developer\": best.record.developer or \"\",\n",
    "#         \"publisher\": best.record.publisher or \"\",\n",
    "#         \"release_date\": best.record.release_date or \"\",\n",
    "#         \"platforms\": \", \".join(best.record.platforms) if best.record.platforms else \"\"\n",
    "#     }\n",
    "#     return resolved\n",
    "\n",
    "    \n",
    "# !pwd\n",
    "# !ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed! Restart kernel now.\n"
     ]
    }
   ],
   "source": [
    "# # Quick fix - add function to workspace\n",
    "# with open('/workspace/Code/project/memory.py', 'a') as f:\n",
    "#     f.write('''\n",
    "\n",
    "# def persist_new_knowledge(records):\n",
    "#     if not records: return 0\n",
    "#     from vector_store import VectorStoreManager\n",
    "#     vsm = VectorStoreManager()\n",
    "#     added = 0\n",
    "#     for record in records:\n",
    "#         try:\n",
    "#             vsm.add_document(doc_id=str(record.id), text=record.title, metadata={\"title\": record.title})\n",
    "#             added += 1\n",
    "#         except: pass\n",
    "#     return added\n",
    "# ''')\n",
    "# print(\"‚úÖ Fixed! Restart kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014f0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ persist_new_knowledge already exists in workspace memory.py\n"
     ]
    }
   ],
   "source": [
    "# Add missing persist_new_knowledge function to workspace memory.py\n",
    "import os\n",
    "\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Check if function already exists\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "if 'def persist_new_knowledge' in content:\n",
    "    print(\"‚úÖ persist_new_knowledge already exists in workspace memory.py\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Function missing. Adding it now...\")\n",
    "    \n",
    "    # Append the function\n",
    "    with open(workspace_memory_path, 'a') as f:\n",
    "        f.write('''\n",
    "\n",
    "# --- Persist new knowledge ---\n",
    "def persist_new_knowledge(records):\n",
    "    \"\"\"\n",
    "    Store web-sourced GameRecord objects into the vector store for future retrieval.\n",
    "    \n",
    "    Args:\n",
    "        records: List of GameRecord objects to persist\n",
    "    \n",
    "    Returns:\n",
    "        Number of records successfully added\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    vsm = VectorStoreManager()\n",
    "    added = 0\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            # Build document text for embedding\n",
    "            doc_text = f\"{record.title}\"\n",
    "            if record.description:\n",
    "                doc_text += f\" - {record.description}\"\n",
    "            if record.developer:\n",
    "                doc_text += f\" | Developer: {record.developer}\"\n",
    "            if record.platforms:\n",
    "                doc_text += f\" | Platforms: {', '.join(record.platforms)}\"\n",
    "            \n",
    "            # Build metadata\n",
    "            metadata = {\n",
    "                \"title\": record.title,\n",
    "                \"developer\": record.developer or \"\",\n",
    "                \"publisher\": record.publisher or \"\",\n",
    "                \"release_date\": record.release_date or \"\",\n",
    "                \"platforms\": \", \".join(record.platforms) if record.platforms else \"\",\n",
    "                \"source\": record.source or \"web\",\n",
    "                \"url\": record.url or \"\",\n",
    "            }\n",
    "            \n",
    "            # Add to vector store\n",
    "            vsm.add_document(doc_id=str(record.id), text=doc_text, metadata=metadata)\n",
    "            added += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to persist record {record.title}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return added\n",
    "''')\n",
    "    \n",
    "    print(\"‚úÖ Function added! Now restart the kernel and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d63bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ persist_new_knowledge function exists\n",
      "Function signature: (records)\n"
     ]
    }
   ],
   "source": [
    "# Check if persist_new_knowledge exists in workspace memory.py\n",
    "import memory\n",
    "import inspect\n",
    "\n",
    "if not hasattr(memory, 'persist_new_knowledge'):\n",
    "    print(\"‚ö†Ô∏è persist_new_knowledge function is missing from memory.py\")\n",
    "    print(f\"Memory module location: {memory.__file__}\")\n",
    "    print(\"\\nYou need to add this function to your workspace memory.py file.\")\n",
    "else:\n",
    "    print(\"‚úÖ persist_new_knowledge function exists\")\n",
    "    print(f\"Function signature: {inspect.signature(memory.persist_new_knowledge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b510e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### UdaPlay Agent Responses ###\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** When Pok√©mon Gold and Silver was released?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Pok√©mon Gold & Silver were released in the US 25 years ago today ...\n",
       "\n",
       "Pok√©mon Gold & Silver were released in the US 25 years ago today üïπÔ∏è ... Since then, no other Pokemon game were trully worth it. ... Best games fr. I\n",
       "\n",
       "[Read more](https://www.threads.com/@culturecrave/post/DP13NppCBdR/video-pok%C3%A9mon-gold-silver-were-released-in-the-us-25-years-ago-today?hl=en)\n",
       "\n",
       "### Source 2: Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago ...\n",
       "\n",
       "Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago these games came out and I still love them. *   Image 2: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Image 3: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? *   Image 4: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Image 5: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver?\n",
       "\n",
       "[Read more](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "\n",
       "### Source 3: Prerelease:Pok√©mon Gold and Silver - The Cutting Room Floor\n",
       "\n",
       "* November 15 - The demos for *Pok√©mon Gold and Silver* at Space World '97 are compiled. * November 21 - *Pok√©mon Gold and Silver* are released in Japan. Given how Dolly's existence was revealed to the world on 22 February 1997, when *Gold and Silver* had already been worked on for a year, it's then possible they originally intended for this Pok√©mon to be included in those games. Additionally, in a November 2009 issue of Nintendo DREAM, game designer Morimoto Shigeki gave more details regarding Lugia's significance in *Gold and Silver*. In this interview, he namely said that while Ho-Oh and Lugia do not have a direct connection in the story, they were envisioned as being \"[...] born in the world of Pok√©mon Gold and Silver for its new feature, the time system, giving us the day and night cycle. ## Region Map. The team originally had troubles figuring out what the world of *Pok√©mon Gold and Silver* would look like.\n",
       "\n",
       "[Read more](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Pok√©mon Gold & Silver were released in the US 25 years ago today ...](https://www.threads.com/@culturecrave/post/DP13NppCBdR/video-pok%C3%A9mon-gold-silver-were-released-in-the-us-25-years-ago-today?hl=en)\n",
       "2. [Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago ...](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "3. [Prerelease:Pok√©mon Gold and Silver - The Cutting Room Floor](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Which one was the first 3D platformer Mario game?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz\n",
       "\n",
       "## Forums - Gaming - Mario 64 wasnt the first true 3D platformer...Sony did it first. Mario 64 was the first \"RENDERED 3D\" game =). But Mario 64 WAS the first 3D platformer to actually BE fully 3D in it's gameplay, and to actually WORK, and play WELL, and be, you know....a fun game to play. This may have beaten Mario to market but while it has 3D graphics, and is a platform game it was never really a 3D platformer. There were quite a few 3d platformers before Mario 64. Mario 64 wasn't praised for being the first 3D platformer. But while Sega and Sony had their own attempts during the same time period at 3D rivals, those being Nights and Crash Bandicoot, both of which were excellent games for their time, Mario 64 was, again, the only one that was TRULY 3D, and it was just doing things that were so far advanced that nothing else could really keep up for many years.\n",
       "\n",
       "[Read more](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "\n",
       "### Source 2: 3D Platform Games - Who Made Them First? - SPINE ONLINE\n",
       "\n",
       "# 3D Platform Games - Who Made Them First? What was the first 3D platformer? In the 80s, platformers with primitive 3D gameplay actually did exist; games like *3-D WorldRunner* attempted to provide a 3D experience on hardware that could only render 2D imagery. But if you‚Äôd consider games like this to be 3D games, then 3D platformers got their start back in the 80s! But despite being the first game to feature many of the genre‚Äôs now-staple mechanics, *Super Mario 64* wasn‚Äôt the first 3D platformer. So while *Alpha Waves* didn‚Äôt have the dramatic, long-term impact on the 3D platformer genre that some later games did, it *was* the very first! One of *Alpha Waves*‚Äôs most obvious differences compared to modern 3D platformers is in the controls. It‚Äôs definitely primitive, even compared to the 3D platformers that came just a few years later, but this control scheme makes *Alpha Waves* a unique experience, even today!\n",
       "\n",
       "[Read more](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "\n",
       "### Source 3: Why is Super Mario 64 known as the 3D revolution in gaming when ...\n",
       "\n",
       "It was known as the 3D revolution in gaming because it was the first 3D game to nail platforming ... Mario 64 was one of the first true 3D games\n",
       "\n",
       "[Read more](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "2. [3D Platform Games - Who Made Them First? - SPINE ONLINE](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "3. [Why is Super Mario 64 known as the 3D revolution in gaming when ...](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Was Mortal Kombat X released for Playstation 5?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mortal Kombat X - PS5 Gameplay - YouTube\n",
       "\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "Section Plays\n",
       "192000 subscribers\n",
       "91 likes\n",
       "10279 views\n",
       "4 Jun 2025\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "\n",
       "About this game :\n",
       "Mortal Kombat X is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. Interactive Entertainment for Microsoft Windows, PlayStation 4, and Xbox One. It is the tenth main installment in the Mortal Kombat series and a sequel to Mortal Kombat (2011), taking place 25 years later after the events of its predecessor. High Voltage Software developed the Windows version of the game, with Polish studio QLOC taking over the work on it shortly after the release of Kombat Pack 1.\n",
       "\n",
       "Who‚Äôs Next? Experience the Next Generation of the 1 Fighting Franchise.\n",
       "\n",
       "Mortal Kombat X combines unparalleled, cinematic presentation with all new gameplay. For the first time, players can choose from multiple variations of each character impacting both strategy and fighting style.\n",
       "\n",
       "#ps5gameplay\n",
       "8 comments\n",
       "\n",
       "\n",
       "[Read more](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "\n",
       "### Source 2: Mortal Kombat X - PlayStation\n",
       "\n",
       "* Supports up to 10 online players with PS Plus. ## Rating and Reviews. Every review comes from a verified owner of this game or item and is evaluated by a team of moderators. Check the Ratings and Reviews Policy for more details. ### Report Review. ### Report Review. Your report will be reviewed by PlayStation Safety, and action will be taken if appropriate. Only owners of this game can rate it. ### No ratings and reviews. Be the first to add a rating and review. ### Your review. ### Thank you for submitting your review. It may take up to 72 hours for your review to be posted. To play this game on PS5, your system may need to be updated to the latest system software. 'MORTAL KOMBAT X software ¬© 2015 Warner Bros. WB GAMES LOGO, WB SHIELD, NETHERREALM LOGO, MORTAL KOMBAT, THE DRAGON LOGO, and all related characters and elements are trademarks of and ¬© Warner Bros.\n",
       "\n",
       "[Read more](https://www.playstation.com/en-us/games/mortal-kombat-x_msm_moved/)\n",
       "\n",
       "### Source 3: Mortal Kombat X - Wikipedia\n",
       "\n",
       "***Mortal Kombat X*** is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. An upgraded version of *Mortal Kombat X*, titled ***Mortal Kombat XL***, was released on March 1, 2016, for PlayStation 4 and Xbox One, including all downloadable content characters from the two released Kombat Packs, almost all bonus alternate costumes available at the time of release, improved gameplay, and improved netcode. By July 2015, due to heavy criticism for the porting issues that plagued the PC release of the game, almost all references to *Mortal Kombat X* had been removed from High Voltage Software's Facebook page. On March 2, 2015, NetherRealm Studios announced that their mobile division would release an iOS/Android \"Android (operating system)\") version of *Mortal Kombat X* in April 2015. With the 1.11 update version of the mobile game released on December 6, 2016, Freddy Krueger who appeared as a DLC character in *MK9 \"Mortal Kombat (2011 video game)\")* was added as a mobile-exclusive character using his signature moves and X-Ray attack from MK9.\n",
       "\n",
       "[Read more](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mortal Kombat X - PS5 Gameplay - YouTube](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "2. [Mortal Kombat X - PlayStation](https://www.playstation.com/en-us/games/mortal-kombat-x_msm_moved/)\n",
       "3. [Mortal Kombat X - Wikipedia](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: Invoke your agent\n",
    "# - When Pok√©mon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X released for Playstation 5?\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Instantiate the agent (defined in previous cell)\n",
    "agent = UdaPlayAgent()\n",
    "\n",
    "# Questions to ask\n",
    "questions = [\n",
    "    \"When Pok√©mon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "]\n",
    "\n",
    "print(\"### UdaPlay Agent Responses ###\\n\")\n",
    "for q in questions:\n",
    "    result = agent.run(q)\n",
    "    display(Markdown(result[\"markdown\"]))\n",
    "    \n",
    "     # TODO: Invoke your agent\n",
    "# # - When Pok√©mon Gold and Silver was released?\n",
    "# # - Which one was the first 3D platformer Mario game?\n",
    "# # - Was Mortal Kombat X realeased for Playstation 5?\n",
    "\n",
    "# # TODO: Invoke your agent\n",
    "# # - When Pok√©mon Gold and Silver was released?\n",
    "# # - Which one was the first 3D platformer Mario game?\n",
    "# # - Was Mortal Kombat X released for Playstation 5?\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "# from agent import UdaPlayAgent  # Ensure agent.py contains the UdaPlayAgent class\n",
    "\n",
    "# # Instantiate the agent\n",
    "# agent = UdaPlayAgent()\n",
    "\n",
    "# # Questions to ask\n",
    "# questions = [\n",
    "#     \"When Pok√©mon Gold and Silver was released?\",\n",
    "#     \"Which one was the first 3D platformer Mario game?\",\n",
    "#     \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "# ]\n",
    "\n",
    "# print(\"### UdaPlay Agent Responses ###\\n\")\n",
    "# for q in questions:\n",
    "#     result = agent.run(q)  # agent.run returns {\"markdown\": render(report)}\n",
    "#     display(Markdown(result[\"markdown\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ PART 2 - TOOL VERIFICATION TEST\n",
      "======================================================================\n",
      "\n",
      "[TEST 1] retrieve_game tool...\n",
      "‚úÖ SUCCESS: Retrieved 3 results\n",
      "   Top result: Super Mario World (Score: 0.503)\n",
      "\n",
      "[TEST 2] evaluate_retrieval tool...\n",
      "‚úÖ SUCCESS: Evaluation complete\n",
      "   Useful: False\n",
      "   Description: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'...\n",
      "\n",
      "[TEST 3] game_web_search tool...\n",
      "‚úÖ SUCCESS: Found 2 web results\n",
      "   Top result: Super Mario 64...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TOOL VERIFICATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "# ============================================================\n",
    "# TEST 1: Verify All Tools Work Individually\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üß™ PART 2 - TOOL VERIFICATION TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: retrieve_game\n",
    "print(\"\\n[TEST 1] retrieve_game tool...\")\n",
    "try:\n",
    "    test_results = retrieve_game(\"Mario platformer game\", n_results=3)\n",
    "    print(f\"‚úÖ SUCCESS: Retrieved {len(test_results)} results\")\n",
    "    print(f\"   Top result: {test_results[0]['Name']} (Score: {test_results[0]['score']:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: {e}\")\n",
    "\n",
    "# Test 2: evaluate_retrieval\n",
    "print(\"\\n[TEST 2] evaluate_retrieval tool...\")\n",
    "try:\n",
    "    eval_report = evaluate_retrieval(\"When was Super Mario 64 released?\", test_results)\n",
    "    print(f\"‚úÖ SUCCESS: Evaluation complete\")\n",
    "    print(f\"   Useful: {eval_report.useful}\")\n",
    "    print(f\"   Description: {eval_report.description[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: {e}\")\n",
    "\n",
    "# Test 3: game_web_search\n",
    "print(\"\\n[TEST 3] game_web_search tool...\")\n",
    "try:\n",
    "    web_results = game_web_search(\"Super Mario 64\", max_results=2)\n",
    "    print(f\"‚úÖ SUCCESS: Found {len(web_results)} web results\")\n",
    "    if web_results and web_results[0]['title'] != \"Web search error\":\n",
    "        print(f\"   Top result: {web_results[0]['title'][:60]}...\")\n",
    "    else:\n",
    "        print(f\"   Note: {web_results[0]['snippet']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TOOL VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec3e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ü§ñ PART 2 - AGENT EXECUTION TEST (Original Questions)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Question 1/3: When Pok√©mon Gold and Silver was released?\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** When Pok√©mon Gold and Silver was released?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Pok√©mon Gold & Silver were released in the US 25 years ago today ...\n",
       "\n",
       "Pok√©mon Gold & Silver were released in the US 25 years ago today üïπÔ∏è ... Since then, no other Pokemon game were trully worth it. ... Best games fr. I\n",
       "\n",
       "[Read more](https://www.threads.com/@culturecrave/post/DP13NppCBdR/video-pok%C3%A9mon-gold-silver-were-released-in-the-us-25-years-ago-today?hl=en)\n",
       "\n",
       "### Source 2: Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago ...\n",
       "\n",
       "Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago these games came out and I still love them. *   Image 2: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Image 3: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? *   Image 4: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver? Image 5: r/Gameboy - Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago which one did you get, Gold or Silver?\n",
       "\n",
       "[Read more](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "\n",
       "### Source 3: Prerelease:Pok√©mon Gold and Silver - The Cutting Room Floor\n",
       "\n",
       "* November 15 - The demos for *Pok√©mon Gold and Silver* at Space World '97 are compiled. * November 21 - *Pok√©mon Gold and Silver* are released in Japan. Given how Dolly's existence was revealed to the world on 22 February 1997, when *Gold and Silver* had already been worked on for a year, it's then possible they originally intended for this Pok√©mon to be included in those games. Additionally, in a November 2009 issue of Nintendo DREAM, game designer Morimoto Shigeki gave more details regarding Lugia's significance in *Gold and Silver*. In this interview, he namely said that while Ho-Oh and Lugia do not have a direct connection in the story, they were envisioned as being \"[...] born in the world of Pok√©mon Gold and Silver for its new feature, the time system, giving us the day and night cycle. ## Region Map. The team originally had troubles figuring out what the world of *Pok√©mon Gold and Silver* would look like.\n",
       "\n",
       "[Read more](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Pok√©mon Gold & Silver were released in the US 25 years ago today ...](https://www.threads.com/@culturecrave/post/DP13NppCBdR/video-pok%C3%A9mon-gold-silver-were-released-in-the-us-25-years-ago-today?hl=en)\n",
       "2. [Pok√©mon Gold and Silver released October 15, 2000 - 21 years ago ...](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "3. [Prerelease:Pok√©mon Gold and Silver - The Cutting Room Floor](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è Response time: 0.42s\n",
      "‚úÖ Question 1 completed successfully\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question 2/3: Which one was the first 3D platformer Mario game?\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Which one was the first 3D platformer Mario game?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz\n",
       "\n",
       "## Forums - Gaming - Mario 64 wasnt the first true 3D platformer...Sony did it first. Mario 64 was the first \"RENDERED 3D\" game =). But Mario 64 WAS the first 3D platformer to actually BE fully 3D in it's gameplay, and to actually WORK, and play WELL, and be, you know....a fun game to play. This may have beaten Mario to market but while it has 3D graphics, and is a platform game it was never really a 3D platformer. There were quite a few 3d platformers before Mario 64. Mario 64 wasn't praised for being the first 3D platformer. But while Sega and Sony had their own attempts during the same time period at 3D rivals, those being Nights and Crash Bandicoot, both of which were excellent games for their time, Mario 64 was, again, the only one that was TRULY 3D, and it was just doing things that were so far advanced that nothing else could really keep up for many years.\n",
       "\n",
       "[Read more](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "\n",
       "### Source 2: 3D Platform Games - Who Made Them First? - SPINE ONLINE\n",
       "\n",
       "# 3D Platform Games - Who Made Them First? What was the first 3D platformer? In the 80s, platformers with primitive 3D gameplay actually did exist; games like *3-D WorldRunner* attempted to provide a 3D experience on hardware that could only render 2D imagery. But if you‚Äôd consider games like this to be 3D games, then 3D platformers got their start back in the 80s! But despite being the first game to feature many of the genre‚Äôs now-staple mechanics, *Super Mario 64* wasn‚Äôt the first 3D platformer. So while *Alpha Waves* didn‚Äôt have the dramatic, long-term impact on the 3D platformer genre that some later games did, it *was* the very first! One of *Alpha Waves*‚Äôs most obvious differences compared to modern 3D platformers is in the controls. It‚Äôs definitely primitive, even compared to the 3D platformers that came just a few years later, but this control scheme makes *Alpha Waves* a unique experience, even today!\n",
       "\n",
       "[Read more](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "\n",
       "### Source 3: Why is Super Mario 64 known as the 3D revolution in gaming when ...\n",
       "\n",
       "It was known as the 3D revolution in gaming because it was the first 3D game to nail platforming ... Mario 64 was one of the first true 3D games\n",
       "\n",
       "[Read more](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "2. [3D Platform Games - Who Made Them First? - SPINE ONLINE](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "3. [Why is Super Mario 64 known as the 3D revolution in gaming when ...](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è Response time: 0.32s\n",
      "‚úÖ Question 2 completed successfully\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Question 3/3: Was Mortal Kombat X released for Playstation 5?\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Was Mortal Kombat X released for Playstation 5?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mortal Kombat X - PS5 Gameplay - YouTube\n",
       "\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "Section Plays\n",
       "192000 subscribers\n",
       "91 likes\n",
       "10279 views\n",
       "4 Jun 2025\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "\n",
       "About this game :\n",
       "Mortal Kombat X is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. Interactive Entertainment for Microsoft Windows, PlayStation 4, and Xbox One. It is the tenth main installment in the Mortal Kombat series and a sequel to Mortal Kombat (2011), taking place 25 years later after the events of its predecessor. High Voltage Software developed the Windows version of the game, with Polish studio QLOC taking over the work on it shortly after the release of Kombat Pack 1.\n",
       "\n",
       "Who‚Äôs Next? Experience the Next Generation of the 1 Fighting Franchise.\n",
       "\n",
       "Mortal Kombat X combines unparalleled, cinematic presentation with all new gameplay. For the first time, players can choose from multiple variations of each character impacting both strategy and fighting style.\n",
       "\n",
       "#ps5gameplay\n",
       "8 comments\n",
       "\n",
       "\n",
       "[Read more](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "\n",
       "### Source 2: Mortal Kombat X - PlayStation\n",
       "\n",
       "* Supports up to 10 online players with PS Plus. ## Rating and Reviews. Every review comes from a verified owner of this game or item and is evaluated by a team of moderators. Check the Ratings and Reviews Policy for more details. ### Report Review. ### Report Review. Your report will be reviewed by PlayStation Safety, and action will be taken if appropriate. Only owners of this game can rate it. ### No ratings and reviews. Be the first to add a rating and review. ### Your review. ### Thank you for submitting your review. It may take up to 72 hours for your review to be posted. To play this game on PS5, your system may need to be updated to the latest system software. 'MORTAL KOMBAT X software ¬© 2015 Warner Bros. WB GAMES LOGO, WB SHIELD, NETHERREALM LOGO, MORTAL KOMBAT, THE DRAGON LOGO, and all related characters and elements are trademarks of and ¬© Warner Bros.\n",
       "\n",
       "[Read more](https://www.playstation.com/en-us/games/mortal-kombat-x_msm_moved/)\n",
       "\n",
       "### Source 3: Mortal Kombat X - Wikipedia\n",
       "\n",
       "***Mortal Kombat X*** is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. An upgraded version of *Mortal Kombat X*, titled ***Mortal Kombat XL***, was released on March 1, 2016, for PlayStation 4 and Xbox One, including all downloadable content characters from the two released Kombat Packs, almost all bonus alternate costumes available at the time of release, improved gameplay, and improved netcode. By July 2015, due to heavy criticism for the porting issues that plagued the PC release of the game, almost all references to *Mortal Kombat X* had been removed from High Voltage Software's Facebook page. On March 2, 2015, NetherRealm Studios announced that their mobile division would release an iOS/Android \"Android (operating system)\") version of *Mortal Kombat X* in April 2015. With the 1.11 update version of the mobile game released on December 6, 2016, Freddy Krueger who appeared as a DLC character in *MK9 \"Mortal Kombat (2011 video game)\")* was added as a mobile-exclusive character using his signature moves and X-Ray attack from MK9.\n",
       "\n",
       "[Read more](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mortal Kombat X - PS5 Gameplay - YouTube](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "2. [Mortal Kombat X - PlayStation](https://www.playstation.com/en-us/games/mortal-kombat-x_msm_moved/)\n",
       "3. [Mortal Kombat X - Wikipedia](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è Response time: 0.53s\n",
      "‚úÖ Question 3 completed successfully\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ AGENT TEST COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TEST 2: Run Agent with Original 3 Questions\n",
    "# ============================================================\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ü§ñ PART 2 - AGENT EXECUTION TEST (Original Questions)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Instantiate agent\n",
    "agent = UdaPlayAgent()\n",
    "\n",
    "# Original questions\n",
    "questions = [\n",
    "    \"When Pok√©mon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "]\n",
    "\n",
    "# Run each question\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question {i}/{len(questions)}: {question}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = agent.run(question)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Display markdown result\n",
    "        display(Markdown(result[\"markdown\"]))\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Response time: {elapsed:.2f}s\")\n",
    "        print(f\"‚úÖ Question {i} completed successfully\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Question {i} FAILED: {e}\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ AGENT TEST COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28679fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üí¨ PART 2 - CONVERSATION STATE TEST\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART A: Current Agent (No Conversation Memory)\n",
      "======================================================================\n",
      "\n",
      "[Question 1]: Which one was the first 3D platformer Mario game?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Which one was the first 3D platformer Mario game?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz\n",
       "\n",
       "## Forums - Gaming - Mario 64 wasnt the first true 3D platformer...Sony did it first. Mario 64 was the first \"RENDERED 3D\" game =). But Mario 64 WAS the first 3D platformer to actually BE fully 3D in it's gameplay, and to actually WORK, and play WELL, and be, you know....a fun game to play. This may have beaten Mario to market but while it has 3D graphics, and is a platform game it was never really a 3D platformer. There were quite a few 3d platformers before Mario 64. Mario 64 wasn't praised for being the first 3D platformer. But while Sega and Sony had their own attempts during the same time period at 3D rivals, those being Nights and Crash Bandicoot, both of which were excellent games for their time, Mario 64 was, again, the only one that was TRULY 3D, and it was just doing things that were so far advanced that nothing else could really keep up for many years.\n",
       "\n",
       "[Read more](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "\n",
       "### Source 2: 3D Platform Games - Who Made Them First? - SPINE ONLINE\n",
       "\n",
       "# 3D Platform Games - Who Made Them First? What was the first 3D platformer? In the 80s, platformers with primitive 3D gameplay actually did exist; games like *3-D WorldRunner* attempted to provide a 3D experience on hardware that could only render 2D imagery. But if you‚Äôd consider games like this to be 3D games, then 3D platformers got their start back in the 80s! But despite being the first game to feature many of the genre‚Äôs now-staple mechanics, *Super Mario 64* wasn‚Äôt the first 3D platformer. So while *Alpha Waves* didn‚Äôt have the dramatic, long-term impact on the 3D platformer genre that some later games did, it *was* the very first! One of *Alpha Waves*‚Äôs most obvious differences compared to modern 3D platformers is in the controls. It‚Äôs definitely primitive, even compared to the 3D platformers that came just a few years later, but this control scheme makes *Alpha Waves* a unique experience, even today!\n",
       "\n",
       "[Read more](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "\n",
       "### Source 3: Why is Super Mario 64 known as the 3D revolution in gaming when ...\n",
       "\n",
       "It was known as the 3D revolution in gaming because it was the first 3D game to nail platforming ... Mario 64 was one of the first true 3D games\n",
       "\n",
       "[Read more](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "2. [3D Platform Games - Who Made Them First? - SPINE ONLINE](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "3. [Why is Super Mario 64 known as the 3D revolution in gaming when ...](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[Question 2 - Follow-up]: What year was that game released?\n",
      "‚ö†Ô∏è NOTE: Current agent has NO memory of previous question!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** What year was that game released?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: What year did this game come out? - Facebook\n",
       "\n",
       "The first version of this game was released in 1990 and the last version was released in 2023. This game is over 33 years old and there are\n",
       "\n",
       "[Read more](https://www.facebook.com/groups/1253503232075082/posts/1908649083227157/)\n",
       "\n",
       "### Source 2: All 62 game years - GameCompanies.com\n",
       "\n",
       "All 62 game years | GameCompanies.com. # Games by Year. ## Discover the release year of your favourite video games with this captivating list. ## This page features 62 years with game releases, dating back to the 70s and earlier. The year 2018 tops the list with a staggering 8,380 video game releases, followed by 2017 with 8,343 games. In third place is the year 2016, with 5,997 games available. 1,182games released in 2023. 1,801games released in 2022. 1,851games released in 2021. 1,990games released in 2020. 5,825games released in 2019. 8,380games released in 2018. 8,343games released in 2017. 5,997games released in 2016. 3,768games released in 2015. 2,522games released in 2014. 1,715games released in 2013. 1,436games released in 2012. 1,484games released in 2011. 1,534games released in 2010. 1,541games released in 2009. 1,217games released in 2008. 1,029games released in 2007. 1,018games released in 2002. 1,080games released in 1995. 1,179games released in 1994. 1,035games released in 1993. Showing 36 of 62 years.\n",
       "\n",
       "[Read more](https://gamecompanies.com/games-by-year)\n",
       "\n",
       "### Source 3: Remember this video game was released in March 2004 : r/farcry\n",
       "\n",
       "Remember this video game was released in March 2004 ¬∑ Comments Section ¬∑ Community Info Section ¬∑ More posts you may like.\n",
       "\n",
       "[Read more](https://www.reddit.com/r/farcry/comments/1ph7f01/remember_this_video_game_was_released_in_march/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [What year did this game come out? - Facebook](https://www.facebook.com/groups/1253503232075082/posts/1908649083227157/)\n",
       "2. [All 62 game years - GameCompanies.com](https://gamecompanies.com/games-by-year)\n",
       "3. [Remember this video game was released in March 2004 : r/farcry](https://www.reddit.com/r/farcry/comments/1ph7f01/remember_this_video_game_was_released_in_march/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ANALYSIS:\n",
      "   ‚ùå Agent couldn't answer because it has no conversation history\n",
      "   ‚ùå 'that game' has no referent in isolation\n",
      "   ‚úÖ This demonstrates the LIMITATION of stateless agents\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PART B: Enhanced Agent (With Conversation Memory)\n",
      "======================================================================\n",
      "\n",
      "[Creating enhanced agent with memory...]\n",
      "\n",
      "[Question 1]: Which one was the first 3D platformer Mario game?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** Which one was the first 3D platformer Mario game?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz\n",
       "\n",
       "## Forums - Gaming - Mario 64 wasnt the first true 3D platformer...Sony did it first. Mario 64 was the first \"RENDERED 3D\" game =). But Mario 64 WAS the first 3D platformer to actually BE fully 3D in it's gameplay, and to actually WORK, and play WELL, and be, you know....a fun game to play. This may have beaten Mario to market but while it has 3D graphics, and is a platform game it was never really a 3D platformer. There were quite a few 3d platformers before Mario 64. Mario 64 wasn't praised for being the first 3D platformer. But while Sega and Sony had their own attempts during the same time period at 3D rivals, those being Nights and Crash Bandicoot, both of which were excellent games for their time, Mario 64 was, again, the only one that was TRULY 3D, and it was just doing things that were so far advanced that nothing else could really keep up for many years.\n",
       "\n",
       "[Read more](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "\n",
       "### Source 2: 3D Platform Games - Who Made Them First? - SPINE ONLINE\n",
       "\n",
       "# 3D Platform Games - Who Made Them First? What was the first 3D platformer? In the 80s, platformers with primitive 3D gameplay actually did exist; games like *3-D WorldRunner* attempted to provide a 3D experience on hardware that could only render 2D imagery. But if you‚Äôd consider games like this to be 3D games, then 3D platformers got their start back in the 80s! But despite being the first game to feature many of the genre‚Äôs now-staple mechanics, *Super Mario 64* wasn‚Äôt the first 3D platformer. So while *Alpha Waves* didn‚Äôt have the dramatic, long-term impact on the 3D platformer genre that some later games did, it *was* the very first! One of *Alpha Waves*‚Äôs most obvious differences compared to modern 3D platformers is in the controls. It‚Äôs definitely primitive, even compared to the 3D platformers that came just a few years later, but this control scheme makes *Alpha Waves* a unique experience, even today!\n",
       "\n",
       "[Read more](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "\n",
       "### Source 3: Why is Super Mario 64 known as the 3D revolution in gaming when ...\n",
       "\n",
       "It was known as the 3D revolution in gaming because it was the first 3D game to nail platforming ... Mario 64 was one of the first true 3D games\n",
       "\n",
       "[Read more](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "2. [3D Platform Games - Who Made Them First? - SPINE ONLINE](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "3. [Why is Super Mario 64 known as the 3D revolution in gaming when ...](https://www.reddit.com/r/gaming/comments/k6wdxs/why_is_super_mario_64_known_as_the_3d_revolution/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[Question 2 - Follow-up]: What year was that game released?\n",
      "‚úÖ Enhanced agent HAS memory of previous question!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# üéÆ UdaPlay Agent Response\n",
       "\n",
       "**Question:** What year was that game released?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: What year did this game come out? - Facebook\n",
       "\n",
       "The first version of this game was released in 1990 and the last version was released in 2023. This game is over 33 years old and there are\n",
       "\n",
       "[Read more](https://www.facebook.com/groups/1253503232075082/posts/1908649083227157/)\n",
       "\n",
       "### Source 2: All 62 game years - GameCompanies.com\n",
       "\n",
       "All 62 game years | GameCompanies.com. # Games by Year. ## Discover the release year of your favourite video games with this captivating list. ## This page features 62 years with game releases, dating back to the 70s and earlier. The year 2018 tops the list with a staggering 8,380 video game releases, followed by 2017 with 8,343 games. In third place is the year 2016, with 5,997 games available. 1,182games released in 2023. 1,801games released in 2022. 1,851games released in 2021. 1,990games released in 2020. 5,825games released in 2019. 8,380games released in 2018. 8,343games released in 2017. 5,997games released in 2016. 3,768games released in 2015. 2,522games released in 2014. 1,715games released in 2013. 1,436games released in 2012. 1,484games released in 2011. 1,534games released in 2010. 1,541games released in 2009. 1,217games released in 2008. 1,029games released in 2007. 1,018games released in 2002. 1,080games released in 1995. 1,179games released in 1994. 1,035games released in 1993. Showing 36 of 62 years.\n",
       "\n",
       "[Read more](https://gamecompanies.com/games-by-year)\n",
       "\n",
       "### Source 3: Remember this video game was released in March 2004 : r/farcry\n",
       "\n",
       "Remember this video game was released in March 2004 ¬∑ Comments Section ¬∑ Community Info Section ¬∑ More posts you may like.\n",
       "\n",
       "[Read more](https://www.reddit.com/r/farcry/comments/1ph7f01/remember_this_video_game_was_released_in_march/)\n",
       "\n",
       "## üìö Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [What year did this game come out? - Facebook](https://www.facebook.com/groups/1253503232075082/posts/1908649083227157/)\n",
       "2. [All 62 game years - GameCompanies.com](https://gamecompanies.com/games-by-year)\n",
       "3. [Remember this video game was released in March 2004 : r/farcry](https://www.reddit.com/r/farcry/comments/1ph7f01/remember_this_video_game_was_released_in_march/)\n",
       "\n",
       "## üîç Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ANALYSIS:\n",
      "   ‚úÖ Agent successfully resolved 'that game' using conversation history\n",
      "   ‚úÖ Follow-up question answered correctly\n",
      "   ‚úÖ This demonstrates STATEFUL conversation capability\n",
      "\n",
      "\n",
      "üìú Conversation History:\n",
      "\n",
      "   Turn 1:\n",
      "      Original: Which one was the first 3D platformer Mario game?\n",
      "\n",
      "   Turn 2:\n",
      "      Original: What year was that game released?\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CONVERSATION STATE TEST COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TEST 3: Conversation State & Follow-Up Question Demo\n",
    "# ============================================================\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üí¨ PART 2 - CONVERSATION STATE TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===========================================================\n",
    "# PART A: Show Current Limitation (No Memory)\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART A: Current Agent (No Conversation Memory)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "agent_stateless = UdaPlayAgent()\n",
    "\n",
    "# Question 1: Ask about first 3D Mario game\n",
    "q1 = \"Which one was the first 3D platformer Mario game?\"\n",
    "print(f\"\\n[Question 1]: {q1}\")\n",
    "result1 = agent_stateless.run(q1)\n",
    "display(Markdown(result1[\"markdown\"]))\n",
    "\n",
    "# Extract answer from result for reference\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Question 2: Follow-up (will FAIL with current implementation)\n",
    "q2_followup = \"What year was that game released?\"\n",
    "print(f\"\\n[Question 2 - Follow-up]: {q2_followup}\")\n",
    "print(\"‚ö†Ô∏è NOTE: Current agent has NO memory of previous question!\\n\")\n",
    "\n",
    "result2 = agent_stateless.run(q2_followup)\n",
    "display(Markdown(result2[\"markdown\"]))\n",
    "\n",
    "print(\"\\nüîç ANALYSIS:\")\n",
    "print(\"   ‚ùå Agent couldn't answer because it has no conversation history\")\n",
    "print(\"   ‚ùå 'that game' has no referent in isolation\")\n",
    "print(\"   ‚úÖ This demonstrates the LIMITATION of stateless agents\\n\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# PART B: Enhanced Agent WITH Conversation Memory\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART B: Enhanced Agent (With Conversation Memory)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class UdaPlayAgentWithMemory(UdaPlayAgent):\n",
    "    \"\"\"Enhanced agent that maintains conversation history\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conversation_history = []  # Store Q&A pairs\n",
    "        self.last_query_context = {}   # Store last query details\n",
    "    \n",
    "    def run(self, question: str) -> Dict[str, str]:\n",
    "        \"\"\"Run agent with conversation memory\"\"\"\n",
    "        \n",
    "        # Try to resolve references like \"that game\", \"it\", etc.\n",
    "        resolved_question = self._resolve_references(question)\n",
    "        \n",
    "        # Run the original agent logic with resolved question\n",
    "        result = super().run(resolved_question)\n",
    "        \n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"original_question\": question,\n",
    "            \"resolved_question\": resolved_question,\n",
    "            \"result\": result\n",
    "        })\n",
    "        \n",
    "        # Update last query context from result\n",
    "        self._extract_context(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _resolve_references(self, question: str) -> str:\n",
    "        \"\"\"Resolve pronouns/references using conversation history\"\"\"\n",
    "        \n",
    "        # Check for reference words\n",
    "        reference_words = [\"that\", \"it\", \"the game\", \"this\"]\n",
    "        has_reference = any(word in question.lower() for word in reference_words)\n",
    "        \n",
    "        if not has_reference or not self.last_query_context:\n",
    "            return question\n",
    "        \n",
    "        # If asking about \"that game\" and we have a game name from last query\n",
    "        if \"that\" in question.lower() and \"game_name\" in self.last_query_context:\n",
    "            game_name = self.last_query_context[\"game_name\"]\n",
    "            resolved = question.replace(\"that game\", game_name)\n",
    "            resolved = resolved.replace(\"That game\", game_name)\n",
    "            print(f\"   üîó Resolved reference: '{question}' ‚Üí '{resolved}'\")\n",
    "            return resolved\n",
    "        \n",
    "        return question\n",
    "    \n",
    "    def _extract_context(self, result: Dict[str, str]) -> None:\n",
    "        \"\"\"Extract key information from result for future reference\"\"\"\n",
    "        markdown = result.get(\"markdown\", \"\")\n",
    "        \n",
    "        # Try to extract game name from markdown\n",
    "        import re\n",
    "        game_match = re.search(r'\\*\\*Game:\\*\\* (.+?)(?:\\n|$)', markdown)\n",
    "        if game_match:\n",
    "            self.last_query_context[\"game_name\"] = game_match.group(1).strip()\n",
    "\n",
    "# Test the enhanced agent\n",
    "print(\"\\n[Creating enhanced agent with memory...]\")\n",
    "agent_with_memory = UdaPlayAgentWithMemory()\n",
    "\n",
    "# Question 1: Ask about first 3D Mario game\n",
    "q1 = \"Which one was the first 3D platformer Mario game?\"\n",
    "print(f\"\\n[Question 1]: {q1}\")\n",
    "result1_mem = agent_with_memory.run(q1)\n",
    "display(Markdown(result1_mem[\"markdown\"]))\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Question 2: Follow-up (will SUCCEED with enhanced agent)\n",
    "q2_followup = \"What year was that game released?\"\n",
    "print(f\"\\n[Question 2 - Follow-up]: {q2_followup}\")\n",
    "print(\"‚úÖ Enhanced agent HAS memory of previous question!\\n\")\n",
    "\n",
    "result2_mem = agent_with_memory.run(q2_followup)\n",
    "display(Markdown(result2_mem[\"markdown\"]))\n",
    "\n",
    "print(\"\\nüîç ANALYSIS:\")\n",
    "print(\"   ‚úÖ Agent successfully resolved 'that game' using conversation history\")\n",
    "print(\"   ‚úÖ Follow-up question answered correctly\")\n",
    "print(\"   ‚úÖ This demonstrates STATEFUL conversation capability\\n\")\n",
    "\n",
    "# Show conversation history\n",
    "print(\"\\nüìú Conversation History:\")\n",
    "for i, entry in enumerate(agent_with_memory.conversation_history, 1):\n",
    "    print(f\"\\n   Turn {i}:\")\n",
    "    print(f\"      Original: {entry['original_question']}\")\n",
    "    if entry['original_question'] != entry['resolved_question']:\n",
    "        print(f\"      Resolved: {entry['resolved_question']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ CONVERSATION STATE TEST COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ce195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
