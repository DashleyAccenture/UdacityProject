{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "# # Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6eac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read current content of workspace memory path into memory\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Read current content\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Removing old version of persist_new_knowledge...\n",
      "âœ… Adding corrected persist_new_knowledge function...\n",
      "âœ… Function added! Restart kernel and try again.\n"
     ]
    }
   ],
   "source": [
    "# Fix: Add or update persist_new_knowledge function in workspace memory.py\n",
    "import os\n",
    "\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Read current content\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Remove old version if it exists\n",
    "if 'def persist_new_knowledge' in content:\n",
    "    print(\"âš ï¸ Removing old version of persist_new_knowledge...\")\n",
    "    # Find and remove the old function\n",
    "    lines = content.split('\\n')\n",
    "    new_lines = []\n",
    "    skip = False\n",
    "    for line in lines:\n",
    "        if 'def persist_new_knowledge' in line:\n",
    "            skip = True\n",
    "        elif skip and line and not line[0].isspace() and line.strip():\n",
    "            skip = False\n",
    "        \n",
    "        if not skip:\n",
    "            new_lines.append(line)\n",
    "    \n",
    "    content = '\\n'.join(new_lines)\n",
    "    with open(workspace_memory_path, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Add the corrected function\n",
    "print(\"âœ… Adding corrected persist_new_knowledge function...\")\n",
    "with open(workspace_memory_path, 'a') as f:\n",
    "    f.write('''\n",
    "\n",
    "# --- Persist new knowledge ---\n",
    "def persist_new_knowledge(records):\n",
    "    \"\"\"\n",
    "    Store web-sourced game records into the vector store for future retrieval.\n",
    "    \n",
    "    Args:\n",
    "        records: List of dict objects with game info to persist\n",
    "    \n",
    "    Returns:\n",
    "        Number of records successfully added\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    from lib.vector_store import VectorStoreManager\n",
    "    vsm = VectorStoreManager()\n",
    "    added = 0\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            # Handle both dict and object types\n",
    "            if isinstance(record, dict):\n",
    "                title = record.get('title', '')\n",
    "                snippet = record.get('snippet', '')\n",
    "                developer = record.get('developer', '')\n",
    "                platforms = record.get('platforms', '')\n",
    "                url = record.get('url', '')\n",
    "            else:\n",
    "                title = getattr(record, 'title', '')\n",
    "                snippet = getattr(record, 'snippet', '')\n",
    "                developer = getattr(record, 'developer', '')\n",
    "                platforms = getattr(record, 'platforms', '')\n",
    "                url = getattr(record, 'url', '')\n",
    "            \n",
    "            # Build document text for embedding\n",
    "            doc_text = f\"{title}\"\n",
    "            if snippet:\n",
    "                doc_text += f\" - {snippet}\"\n",
    "            if developer:\n",
    "                doc_text += f\" | Developer: {developer}\"\n",
    "            if platforms:\n",
    "                doc_text += f\" | Platforms: {platforms}\"\n",
    "            \n",
    "            # Build metadata\n",
    "            metadata = {\n",
    "                \"title\": title,\n",
    "                \"developer\": developer or \"\",\n",
    "                \"platforms\": platforms or \"\",\n",
    "                \"source\": \"web\",\n",
    "                \"url\": url or \"\",\n",
    "            }\n",
    "            \n",
    "            # Add to vector store\n",
    "            import hashlib\n",
    "            doc_id = hashlib.md5(title.encode()).hexdigest()\n",
    "            vsm.add_document(doc_id=doc_id, text=doc_text, metadata=metadata)\n",
    "            added += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to persist record: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return added\n",
    "''')\n",
    "\n",
    "print(\"âœ… Function added! Restart kernel and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7613211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… persist_new_knowledge function exists\n",
      "Function signature: (records)\n"
     ]
    }
   ],
   "source": [
    "# Check if persist_new_knowledge exists in workspace memory.py\n",
    "import memory\n",
    "import inspect\n",
    "\n",
    "if not hasattr(memory, 'persist_new_knowledge'):\n",
    "    print(\"âš ï¸ persist_new_knowledge function is missing from memory.py\")\n",
    "    print(f\"Memory module location: {memory.__file__}\")\n",
    "    print(\"\\nYou need to add this function to your workspace memory.py file.\")\n",
    "else:\n",
    "    print(\"âœ… persist_new_knowledge function exists\")\n",
    "    print(f\"Function signature: {inspect.signature(memory.persist_new_knowledge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce74db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# # Connect to the persistent ChromaDB created in Part 1\n",
    "# chroma_client = chromadb(path=\"chromadb\")\n",
    "\n",
    "# # Get the embedding function (must match Part 1)\n",
    "# embedding_fn = OpenAIEmbeddingFunction(\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "# # Get the existing collection\n",
    "# collection = chroma_client.get_collection(\n",
    "#     name=\"udaplay\", \n",
    "#     embedding_function=embedding_fn\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… Connected to collection: {collection.name}\")\n",
    "# print(f\"   Documents in collection: {collection.count()}\")\n",
    "\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# # Connect to the persistent ChromaDB created in Part 1\n",
    "# chroma_client = chromadb.PersistentClient(path=\"chromadb\")  # â† Fixed: was missing .PersistentClient\n",
    "\n",
    "# # Get the embedding function (must match Part 1)\n",
    "# embedding_fn = OpenAIEmbeddingFunction(\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "#     model_name=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "# # Get the existing collection\n",
    "# collection = chroma_client.get_collection(\n",
    "#     name=\"udaplay\", \n",
    "#     embedding_function=embedding_fn\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… Connected to collection: {collection.name}\")\n",
    "# print(f\"   Documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ac549f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GameRecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: Import the necessary libs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# For example: \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserMessage, SystemMessage, ToolMessage, AIMessage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/project/lib/agents.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIMessage, UserMessage, SystemMessage, ToolMessage\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtooling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool, ToolCall\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShortTermMemory\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define the state schema\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAgentState\u001b[39;00m(TypedDict):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Code/project/lib/memory.py:354\u001b[39m\n\u001b[32m    347\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m MemorySearchResult(\n\u001b[32m    348\u001b[39m             fragments=fragments,\n\u001b[32m    349\u001b[39m             metadata=result_metadata\n\u001b[32m    350\u001b[39m         )\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# --- Persist new knowledge ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpersist_new_knowledge\u001b[39m(records: List[\u001b[43mGameRecord\u001b[49m]) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    355\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[33;03m    Store web-sourced GameRecord objects into the vector store for future retrieval.\u001b[39;00m\n\u001b[32m    357\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m \u001b[33;03m        Number of records successfully added\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m records:\n",
      "\u001b[31mNameError\u001b[39m: name 'GameRecord' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "import os\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from lib.tooling import tool\n",
    "from dotenv import load_dotenv\n",
    "# from enum import Enum, auto\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "tavily_key = TAVILY_API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d80a7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to collection: udaplay\n",
      "   Documents in collection: 15\n"
     ]
    }
   ],
   "source": [
    "## added this code here toreconnect to embeddings from part 1 and proceed in the correct order.\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# Connect to the persistent ChromaDB created in Part 1\n",
    "chroma_client = chromadb.PersistentClient(path=\"chromadb\")  # â† Fixed: was missing .PersistentClient\n",
    "\n",
    "# Get the embedding function (must match Part 1)\n",
    "embedding_fn = OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Get the existing collection\n",
    "collection = chroma_client.get_collection(\n",
    "    name=\"udaplay\", \n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "print(f\"âœ… Connected to collection: {collection.name}\")\n",
    "print(f\"   Documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "##chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "##collection = chroma_client.get_collection(name=\"udaplay\", embedding_function=embedding_fn)\n",
    "\n",
    "##Retrieve_game tool\n",
    "def retrieve_game(query: str, n_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Semantic search: Finds most relevant results in the vector DB.\n",
    "\n",
    "    Args:\n",
    "        query: A question or description about games (platforms, names, years, etc.).\n",
    "        n_results: How many top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts, each containing:\n",
    "            - Platform\n",
    "            - Name\n",
    "            - YearOfRelease\n",
    "            - Description\n",
    "            - id (the Chroma document id)\n",
    "            - score (similarity score derived from distance; higher is better)\n",
    "    \"\"\"\n",
    "    if not query or not isinstance(query, str):\n",
    "        raise ValueError(\"`query` must be a non-empty string.\")\n",
    "\n",
    "    # Chroma query returns lists grouped by each input query_texts item.\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "         include=[\"metadatas\", \"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Defensive extraction: Chroma returns nested lists, one per query.\n",
    "    metadatas = res.get(\"metadatas\", [[]])[0]\n",
    "    documents = res.get(\"documents\", [[]])[0]\n",
    "    ids       = res.get(\"ids\", [[]])[0]\n",
    "    distances = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(ids)):\n",
    "        meta = metadatas[i] if i < len(metadatas) else {}\n",
    "        doc  = documents[i] if i < len(documents) else \"\"\n",
    "        dist = distances[i] if i < len(distances) else None\n",
    "\n",
    "        # Convert Chroma distance to a 0..1 similarity score (heuristic).\n",
    "        # Chroma's \"distance\" is typically cosine distance (0 is identical).\n",
    "        # We map it to score = 1 - min(max(dist,0),1). If dist > 1, clamp to 0.\n",
    "        if dist is None:\n",
    "            score = None\n",
    "        else:\n",
    "            score = 1.0 - max(0.0, min(float(dist), 1.0))\n",
    "\n",
    "        # Normalize metadata keys expected from your add loop\n",
    "        results.append({\n",
    "            \"Platform\":     meta.get(\"Platform\"),\n",
    "            \"Name\":         meta.get(\"Name\"),\n",
    "            \"YearOfRelease\": meta.get(\"YearOfRelease\"),\n",
    "            \"Description\":  meta.get(\"Description\"),\n",
    "            \"id\":           ids[i],\n",
    "            \"score\":        score,\n",
    "            # Optional: include the raw document string\n",
    "            \"document\":     doc\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result\n",
    "\n",
    "##Evaluate_retrieval:\n",
    "\n",
    "# tools_evaluate.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Data class for the LLM judge outcome.\n",
    "    - useful: whether the documents are useful to answer the question\n",
    "    - description: detailed explanation supporting the decision\n",
    "    \"\"\"\n",
    "    useful: bool\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Initialize environment and client once\n",
    "load_dotenv('.env')\n",
    "_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "_OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\")  # optional (e.g., Azure/OpenAI proxy)\n",
    "_MODEL_NAME = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "if not _OPENAI_API_KEY:\n",
    "    # Fail fast with a clear message; Udacity runner will surface this\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is missing in .env\")\n",
    "\n",
    "_client = OpenAI(api_key=_OPENAI_API_KEY, base_url=_OPENAI_BASE_URL)\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    retrieved_docs: List[Dict[str, Any]],\n",
    "    max_docs: int = 8,\n",
    ") -> EvaluationReport:\n",
    "    \"\"\"\n",
    "    Tool: evaluate_retrieval\n",
    "    ------------------------\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    Args:\n",
    "        - question: original question from user\n",
    "        - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    Returns:\n",
    "        EvaluationReport:\n",
    "            - useful: whether the documents are useful to answer the question\n",
    "            - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        return EvaluationReport(useful=False, description=\"Invalid question.\")\n",
    "    if not retrieved_docs:\n",
    "        return EvaluationReport(useful=False, description=\"No documents retrieved to evaluate.\")\n",
    "\n",
    "    # Compact, LLM-friendly view of docs (limit to max_docs, truncate long descriptions)\n",
    "    lines: List[str] = []\n",
    "    for i, d in enumerate(retrieved_docs[:max_docs], start=1):\n",
    "        name = str(d.get(\"Name\", \"Unknown\"))\n",
    "        plat = str(d.get(\"Platform\", \"\"))\n",
    "        year = str(d.get(\"YearOfRelease\", \"\"))\n",
    "        desc = str(d.get(\"Description\", \"\"))[:500]  # keep prompt size reasonable\n",
    "        score = d.get(\"score\")\n",
    "        doc_id = d.get(\"id\")\n",
    "        lines.append(\n",
    "            f\"Doc {i}: Name={name}; Platform={plat}; Year={year}; Score={score}; Id={doc_id}; Description={desc}\"\n",
    "        )\n",
    "    docs_block = \"\\n\".join(lines)\n",
    "\n",
    "    # LLM judge prompt per TODO\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator.\n",
    "Your task is to evaluate if the provided documents are enough to respond to the query.\n",
    "\n",
    "Query:\n",
    "\\\"\\\"\\\"{question}\\\"\\\"\\\"\n",
    "\n",
    "Documents:\n",
    "{docs_block}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the documents, as a set, are sufficient and relevant to answer the query.\n",
    "- Consider coverage of key facts the query implies (e.g., developer, release date, platform), when applicable.\n",
    "- If not sufficient, explain what's missing or ambiguous.\n",
    "- Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond ONLY in strict JSON with the following keys:\n",
    "- \"useful\": true or false\n",
    "- \"description\": a concise but informative explanation\n",
    "\"\"\"\n",
    "\n",
    "   \n",
    "    # Call the model; enforce JSON output if supported by your SDK version\n",
    "    try:\n",
    "        response = _client.chat.completions.create(\n",
    "            model=_MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},  # helps ensure valid JSON\n",
    "        )\n",
    "        raw_text = response.choices[0].message.content.strip()  \n",
    "\n",
    "    # Call the model; enforce JSON output if supported by your SDK version    \n",
    "    # try:\n",
    "    #     response = _client.responses.create(\n",
    "    #         model=_MODEL_NAME,\n",
    "    #         input=prompt,\n",
    "    #         response_format={\"type\": \"json_object\"},  # helps ensure valid JSON\n",
    "    #     )\n",
    "    #     raw_text = response.output_text.strip()\n",
    "    except Exception as e:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM evaluation failed: {e}\"\n",
    "        )\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(raw_text)\n",
    "        useful = bool(parsed.get(\"useful\"))\n",
    "        description = str(parsed.get(\"description\", \"\")).strip() or \"No description provided.\"\n",
    "        return EvaluationReport(useful=useful, description=description)\n",
    "    except Exception:\n",
    "        # Fallback if model returned non-JSON\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM response could not be parsed as JSON: {raw_text}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "query = \"in what year was need for speed first sold and by whom?\"\n",
    "\n",
    "##Game web Search - External search helper (graceful fallback)\n",
    "\n",
    "# def game_web_search(query: str, max_results: int = 5) -> list[dict]:\n",
    "#     \"\"\"\n",
    "#     Lightweight web search helper for game-related queries.\n",
    "#     Tries to fetch simple web results. Falls back gracefully if no internet.\n",
    "\n",
    "#     Args:\n",
    "#         query: Search query (e.g., game name + platform + release year).\n",
    "#         max_results: Max items to return.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of dicts: { \"title\": str, \"url\": str, \"snippet\": str }.\n",
    "#         If web access is unavailable, returns an empty list and a note in 'snippet'.\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "query = \"in what year was need for speed first sold and by whom?\"\n",
    "\n",
    "##Game web Search - External search helper (graceful fallback)\n",
    "\n",
    "def game_web_search(query: str, max_results: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Lightweight web search helper for game-related queries.\n",
    "    Uses Tavily for web search.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (e.g., game name + platform + release year).\n",
    "        max_results: Max items to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of dicts: { \"title\": str, \"url\": str, \"snippet\": str, \"developer\": str, \"publisher\": str, \"release_date\": str, \"platforms\": str }.\n",
    "    \"\"\"\n",
    "    from tavily import TavilyClient\n",
    "\n",
    "    try:\n",
    "        client = TavilyClient(api_key=tavily_key)\n",
    "        response = client.search(query, max_results=max_results)\n",
    "        results = []\n",
    "        for r in response['results']:\n",
    "            results.append({\n",
    "                \"title\": r['title'],\n",
    "                \"url\": r['url'],\n",
    "                \"snippet\": r['content'],\n",
    "                \"developer\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"release_date\": \"\",\n",
    "                \"platforms\": \"\"\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"title\": \"Web search error\",\n",
    "            \"url\": \"\",\n",
    "            \"snippet\": f\"Exception during web search: {e}\",\n",
    "            \"developer\": \"\",\n",
    "            \"publisher\": \"\",\n",
    "            \"release_date\": \"\",\n",
    "            \"platforms\": \"\"\n",
    "        }]\n",
    "\n",
    "# Eval retrieval tool\n",
    "def evaluate_retrieval(\n",
    "    question: str,\n",
    "    retrieved_docs: List[Dict[str, Any]],\n",
    "    max_docs: int = 8,\n",
    ") -> EvaluationReport:\n",
    "    \"\"\"\n",
    "    Tool: evaluate_retrieval\n",
    "    ------------------------\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    Args:\n",
    "        - question: original question from user\n",
    "        - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    Returns:\n",
    "        EvaluationReport:\n",
    "            - useful: whether the documents are useful to answer the question\n",
    "            - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(question, str) or not question.strip():\n",
    "        return EvaluationReport(useful=False, description=\"Invalid question.\")\n",
    "    if not retrieved_docs:\n",
    "        return EvaluationReport(useful=False, description=\"No documents retrieved to evaluate.\")\n",
    "\n",
    "    # Compact, LLM-friendly view of docs\n",
    "    lines: List[str] = []\n",
    "    for i, d in enumerate(retrieved_docs[:max_docs], start=1):\n",
    "        name = str(d.get(\"Name\", \"Unknown\"))\n",
    "        plat = str(d.get(\"Platform\", \"\"))\n",
    "        year = str(d.get(\"YearOfRelease\", \"\"))\n",
    "        desc = str(d.get(\"Description\", \"\"))[:500]\n",
    "        score = d.get(\"score\")\n",
    "        doc_id = d.get(\"id\")\n",
    "        lines.append(\n",
    "            f\"Doc {i}: Name={name}; Platform={plat}; Year={year}; Score={score}; Id={doc_id}; Description={desc}\"\n",
    "        )\n",
    "    docs_block = \"\\n\".join(lines)\n",
    "\n",
    "    # LLM judge prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator.\n",
    "Your task is to evaluate if the provided documents are enough to respond to the query.\n",
    "\n",
    "Query:\n",
    "\\\"\\\"\\\"{question}\\\"\\\"\\\"\n",
    "\n",
    "Documents:\n",
    "{docs_block}\n",
    "\n",
    "Instructions:\n",
    "- Determine if the documents, as a set, are sufficient and relevant to answer the query.\n",
    "- Consider coverage of key facts the query implies (e.g., developer, release date, platform), when applicable.\n",
    "- If not sufficient, explain what's missing or ambiguous.\n",
    "- Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond ONLY in strict JSON with the following keys:\n",
    "- \"useful\": true or false\n",
    "- \"description\": a concise but informative explanation\n",
    "\"\"\"\n",
    "\n",
    "    # Call the model\n",
    "    try:\n",
    "        response = _client.responses.create(\n",
    "            model=_MODEL_NAME,\n",
    "            input=prompt,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        raw_text = response.output_text.strip()\n",
    "    except Exception as e:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM evaluation failed: {e}\"\n",
    "        )\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        parsed = json.loads(raw_text)\n",
    "        useful = bool(parsed.get(\"useful\"))\n",
    "        description = str(parsed.get(\"description\", \"\")).strip() or \"No description provided.\"\n",
    "        return EvaluationReport(useful=useful, description=description)\n",
    "    except Exception:\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=f\"LLM response could not be parsed as JSON: {raw_text}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game\n",
    "\n",
    "  \n",
    "  ## beautiful soup implementation.\n",
    "\n",
    "  \n",
    "  \n",
    "    # try:\n",
    "    #     import requests\n",
    "    #     # from bs4 import BeautifulSoup  # requires 'beautifulsoup4' installed\n",
    "    # except Exception:\n",
    "    #     # Fallback (no web libs)\n",
    "    #     return [{\n",
    "    #         \"title\": \"Web search unavailable\",\n",
    "    #         \"url\": \"\",\n",
    "    #         \"snippet\": \"Requests/BeautifulSoup not available in this environment.\"\n",
    "    #     }]\n",
    "\n",
    "#     try:\n",
    "#         # Very simple HTML search using DuckDuckGo (no API key)\n",
    "#         resp = requests.get(\"https://duckduckgo.com/html/\", params={\"q\": query}, timeout=8)\n",
    "#         if resp.status_code != 200:\n",
    "#             return [{\n",
    "#                 \"title\": \"Web search failed\",\n",
    "#                 \"url\": \"\",\n",
    "#                 \"snippet\": f\"HTTP {resp.status_code} while searching for '{query}'.\"\n",
    "#             }]\n",
    "#         soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#         results = []\n",
    "#         for a in soup.select(\".result__a\")[:max_results]:\n",
    "#             title = a.get_text(strip=True)\n",
    "#             url = a.get(\"href\", \"\")\n",
    "#             snippet_tag = a.find_parent(\"div\", class_=\"result\").select_one(\".result__snippet\")\n",
    "#             snippet = snippet_tag.get_text(strip=True) if snippet_tag else \"\"\n",
    "#             results.append({\"title\": title, \"url\": url, \"snippet\": snippet})\n",
    "#         if not results:\n",
    "#             results.append({\"title\": \"No results parsed\", \"url\": \"\", \"snippet\": \"Parsing returned no items.\"})\n",
    "#         return results\n",
    "#     except Exception as e:\n",
    "#         return [{\n",
    "#             \"title\": \"Web search error\",\n",
    "#             \"url\": \"\",\n",
    "#             \"snippet\": f\"Exception during web search: {e}\"\n",
    "#         }]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Tool Docstring:\n",
    "# #    Semantic search: Finds most results in the vector DB\n",
    "# #    args:\n",
    "# #    - query: a question about game industry. \n",
    "# #\n",
    "# #    You'll receive results as list. Each element contains:\n",
    "# #    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "# #    - Name: Name of the Game\n",
    "# #    - YearOfRelease: Year when that game was released for that platform\n",
    "# #    - Description: Additional details about the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # test_evaluate.py\n",
    "# from tools_retrieve import retrieve_game\n",
    "# # from tools_evaluate import evaluate_retrieval\n",
    "\n",
    "# query = 'Who developed \"FIFA 21\"?'\n",
    "# results = retrieve_game(query, n_results=5)\n",
    "\n",
    "# print(\"\\nRetrieved Results:\")\n",
    "# for r in results:\n",
    "#     print(f\"{r['Name']} | score={r['score']} | Year={r['YearOfRelease']}\")\n",
    "\n",
    "# print(\"\\nEvaluation Metrics:\")\n",
    "# metrics = evaluate_retrieval(query, results)\n",
    "# for k, v in metrics.items():\n",
    "#     print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed\n",
    "\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List\n",
    "\n",
    "class AgentState(Enum):\n",
    "    ASK = auto()\n",
    "    RAG = auto()\n",
    "    EVAL = auto()\n",
    "    WEB = auto()\n",
    "    REPORT = auto()\n",
    "\n",
    "class UdaPlayAgent:\n",
    "    def __init__(self):\n",
    "        self.min_confidence_threshold = 0.6\n",
    "    \n",
    "    def run(self, question: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Run the agent through its state machine to answer a game industry question.\n",
    "        \n",
    "        States:\n",
    "        1. ASK - Receive question\n",
    "        2. RAG - Retrieve from vector DB\n",
    "        3. EVAL - Evaluate if RAG results are sufficient\n",
    "        4. WEB - If needed, search web\n",
    "        5. REPORT - Generate final answer\n",
    "        \"\"\"\n",
    "        state = AgentState.ASK\n",
    "        reasoning_steps = []\n",
    "        \n",
    "        # State: RAG - Retrieve from local vector DB\n",
    "        state = AgentState.RAG\n",
    "        rag_results = retrieve_game(question, n_results=5)\n",
    "        reasoning_steps.append(f\"Retrieved {len(rag_results)} documents from vector DB\")\n",
    "        \n",
    "        # State: EVAL - Evaluate retrieval quality\n",
    "        state = AgentState.EVAL\n",
    "        eval_report = evaluate_retrieval(question, rag_results)\n",
    "        reasoning_steps.append(f\"Evaluation: {eval_report.description}\")\n",
    "        \n",
    "        # State: WEB - Search web if RAG not sufficient\n",
    "        web_results = []\n",
    "        if not eval_report.useful:\n",
    "            state = AgentState.WEB\n",
    "            web_results = game_web_search(question, max_results=3)\n",
    "            reasoning_steps.append(f\"Used web search, found {len(web_results)} results\")\n",
    "        \n",
    "        # State: REPORT - Generate markdown report\n",
    "        state = AgentState.REPORT\n",
    "        markdown_report = self._build_markdown_report(\n",
    "            question=question,\n",
    "            rag_results=rag_results,\n",
    "            web_results=web_results,\n",
    "            eval_report=eval_report,\n",
    "            reasoning=reasoning_steps\n",
    "        )\n",
    "        \n",
    "        return {\"markdown\": markdown_report}\n",
    "    \n",
    "    def _build_markdown_report(\n",
    "        self, \n",
    "        question: str, \n",
    "        rag_results: List[Dict], \n",
    "        web_results: List[Dict],\n",
    "        eval_report,\n",
    "        reasoning: List[str]\n",
    "    ) -> str:\n",
    "        \"\"\"Build a markdown report from agent results.\"\"\"\n",
    "        \n",
    "        # Header\n",
    "        report = f\"# ðŸŽ® UdaPlay Agent Response\\n\\n\"\n",
    "        report += f\"**Question:** {question}\\n\\n\"\n",
    "        report += f\"**Confidence:** {'High' if eval_report.useful else 'Low (used web search)'}\\n\\n\"\n",
    "        \n",
    "        # Answer section\n",
    "        report += \"## Answer\\n\\n\"\n",
    "        \n",
    "        if eval_report.useful and rag_results:\n",
    "            # Answer from RAG\n",
    "            best_match = rag_results[0]\n",
    "            report += f\"**Game:** {best_match['Name']}\\n\\n\"\n",
    "            report += f\"**Platform:** {best_match.get('Platform', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Year:** {best_match.get('YearOfRelease', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Description:** {best_match.get('Description', 'N/A')}\\n\\n\"\n",
    "            report += f\"**Relevance Score:** {best_match.get('score', 0):.3f}\\n\\n\"\n",
    "        elif web_results:\n",
    "            # Answer from web\n",
    "            for i, result in enumerate(web_results[:3], 1):\n",
    "                report += f\"### Source {i}: {result['title']}\\n\\n\"\n",
    "                report += f\"{result['snippet']}\\n\\n\"\n",
    "                report += f\"[Read more]({result['url']})\\n\\n\"\n",
    "        else:\n",
    "            report += \"No sufficient information found to answer this question.\\n\\n\"\n",
    "        \n",
    "        # Sources section\n",
    "        report += \"## ðŸ“š Sources\\n\\n\"\n",
    "        \n",
    "        if eval_report.useful:\n",
    "            report += \"### Vector Database Results\\n\\n\"\n",
    "            for i, doc in enumerate(rag_results[:3], 1):\n",
    "                report += f\"{i}. **{doc['Name']}** ({doc.get('YearOfRelease', 'N/A')}) - \"\n",
    "                report += f\"Score: {doc.get('score', 0):.3f}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        if web_results:\n",
    "            report += \"### Web Search Results\\n\\n\"\n",
    "            for i, result in enumerate(web_results[:3], 1):\n",
    "                report += f\"{i}. [{result['title']}]({result['url']})\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Reasoning section\n",
    "        report += \"## ðŸ” Agent Reasoning\\n\\n\"\n",
    "        for i, step in enumerate(reasoning, 1):\n",
    "            report += f\"{i}. {step}\\n\"\n",
    "        \n",
    "        return report\n",
    "        \n",
    "        # # TODO: Create your Agent abstraction using StateMachine\n",
    "# # Equip with an appropriate model\n",
    "# # Craft a good set of instructions \n",
    "# # Plug all Tools you developed\n",
    "# from enum import Enum, auto\n",
    "# from typing import Dict, List\n",
    "\n",
    "# class AgentState(Enum):\n",
    "#     ASK = auto()\n",
    "#     RAG = auto()\n",
    "#     EVAL = auto()\n",
    "#     WEB = auto()\n",
    "#     PARSE = auto()\n",
    "#     STORE = auto()\n",
    "#     REPORT = auto()\n",
    "\n",
    "# class UdaPlayAgent:\n",
    "#     def __init__(self): pass\n",
    "#        self.min_confidence_threshold = 0.6\n",
    "\n",
    "#     def run(self, question: str) -> Dict[str, str]:\n",
    "#         state = AgentState.ASK\n",
    "#         rag_hits: List[RetrievalHit] = []\n",
    "#         web_records: List[GameRecord] = []\n",
    "#         confidence = 0.0\n",
    "#         metrics = {}\n",
    "#         resolved: Dict[str, str] = {}\n",
    "#         reasoning_steps: List[str] = []\n",
    "\n",
    "#         # RAG\n",
    "#         state = AgentState.RAG\n",
    "#         rag_hits = retrieve_game(question)\n",
    "#         reasoning_steps.append(f\"RAG returned {len(rag_hits)} hits.\")\n",
    "\n",
    "#         # Evaluate\n",
    "#         state = AgentState.EVAL\n",
    "#         confidence, metrics = evaluate_retrieval(question, rag_hits)\n",
    "#         reasoning_steps.append(f\"Evaluation metrics: {metrics} â†’ confidence={confidence:.3f}.\")\n",
    "\n",
    "#         # Decide fallback\n",
    "#         if confidence < MIN_CONFIDENCE:\n",
    "#             state = AgentState.WEB\n",
    "#             web_records = game_web_search(question)\n",
    "#             reasoning_steps.append(f\"Fallback to web produced {len(web_records)} candidates.\")\n",
    "#             state = AgentState.PARSE\n",
    "#             # prefer first web record; refine resolved fields\n",
    "#             best_web = web_records[0] if web_records else None\n",
    "#             if best_web:\n",
    "#                 resolved = {\n",
    "#                     \"title\": best_web.title or _infer_title(question),\n",
    "#                     \"developer\": best_web.developer or \"\",\n",
    "#                     \"publisher\": best_web.publisher or \"\",\n",
    "#                     \"release_date\": best_web.release_date or \"\",\n",
    "#                     \"platforms\": \", \".join(best_web.platforms) if best_web.platforms else \"\"\n",
    "#                 }\n",
    "#                 # store memory\n",
    "#                 state = AgentState.STORE\n",
    "#                 added = persist_new_knowledge(web_records[:3])\n",
    "#                 reasoning_steps.append(f\"Persisted {added} new webâ€‘sourced records.\")\n",
    "#                 # recompute confidence (boost slightly due to fresh authoritative source)\n",
    "#                 confidence = min(1.0, max(confidence, 0.72))\n",
    "#         else:\n",
    "#             # Resolve from local hits (majority vote on top 3)\n",
    "#             resolved = _resolve_from_local(question, rag_hits[:3])\n",
    "#             reasoning_steps.append(\"Resolved facts from local dataset.\")\n",
    "\n",
    "#         state = AgentState.REPORT\n",
    "#         from report import render\n",
    "#         report = build_report(\n",
    "#             question=question,\n",
    "#             resolved=resolved,\n",
    "#             confidence=confidence,\n",
    "#             sources_local=rag_hits,\n",
    "#             sources_web=web_records,\n",
    "#             reasoning=\" â†’ \".join(reasoning_steps)\n",
    "#         )\n",
    "#         return {\"markdown\": render(report)}\n",
    "\n",
    "# def _infer_title(question: str) -> str:\n",
    "#     import re\n",
    "#     m = re.search(r\"â€œ([^â€]+)â€|\\\"([^\\\"]+)\\\"\", question)\n",
    "#     return m.group(1) or m.group(2) if m else \"\"\n",
    "\n",
    "# def _resolve_from_local(question: str, hits: List[RetrievalHit]) -> Dict[str, str]:\n",
    "#     # choose best hit and pull structured fields (simple heuristic: highest score)\n",
    "#     if not hits: return {}\n",
    "#     best = sorted(hits, key=lambda h: h.score, reverse=True)[0]\n",
    "#     resolved = {\n",
    "#         \"title\": best.title,\n",
    "#         \"developer\": best.record.developer or \"\",\n",
    "#         \"publisher\": best.record.publisher or \"\",\n",
    "#         \"release_date\": best.record.release_date or \"\",\n",
    "#         \"platforms\": \", \".join(best.record.platforms) if best.record.platforms else \"\"\n",
    "#     }\n",
    "#     return resolved\n",
    "\n",
    "    \n",
    "# !pwd\n",
    "# !ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed! Restart kernel now.\n"
     ]
    }
   ],
   "source": [
    "# # Quick fix - add function to workspace\n",
    "# with open('/workspace/Code/project/memory.py', 'a') as f:\n",
    "#     f.write('''\n",
    "\n",
    "# def persist_new_knowledge(records):\n",
    "#     if not records: return 0\n",
    "#     from vector_store import VectorStoreManager\n",
    "#     vsm = VectorStoreManager()\n",
    "#     added = 0\n",
    "#     for record in records:\n",
    "#         try:\n",
    "#             vsm.add_document(doc_id=str(record.id), text=record.title, metadata={\"title\": record.title})\n",
    "#             added += 1\n",
    "#         except: pass\n",
    "#     return added\n",
    "# ''')\n",
    "# print(\"âœ… Fixed! Restart kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… persist_new_knowledge already exists in workspace memory.py\n"
     ]
    }
   ],
   "source": [
    "# Add missing persist_new_knowledge function to workspace memory.py\n",
    "import os\n",
    "\n",
    "workspace_memory_path = '/workspace/Code/project/memory.py'\n",
    "\n",
    "# Check if function already exists\n",
    "with open(workspace_memory_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "if 'def persist_new_knowledge' in content:\n",
    "    print(\"âœ… persist_new_knowledge already exists in workspace memory.py\")\n",
    "else:\n",
    "    print(\"âš ï¸ Function missing. Adding it now...\")\n",
    "    \n",
    "    # Append the function\n",
    "    with open(workspace_memory_path, 'a') as f:\n",
    "        f.write('''\n",
    "\n",
    "# --- Persist new knowledge ---\n",
    "def persist_new_knowledge(records):\n",
    "    \"\"\"\n",
    "    Store web-sourced GameRecord objects into the vector store for future retrieval.\n",
    "    \n",
    "    Args:\n",
    "        records: List of GameRecord objects to persist\n",
    "    \n",
    "    Returns:\n",
    "        Number of records successfully added\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    vsm = VectorStoreManager()\n",
    "    added = 0\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            # Build document text for embedding\n",
    "            doc_text = f\"{record.title}\"\n",
    "            if record.description:\n",
    "                doc_text += f\" - {record.description}\"\n",
    "            if record.developer:\n",
    "                doc_text += f\" | Developer: {record.developer}\"\n",
    "            if record.platforms:\n",
    "                doc_text += f\" | Platforms: {', '.join(record.platforms)}\"\n",
    "            \n",
    "            # Build metadata\n",
    "            metadata = {\n",
    "                \"title\": record.title,\n",
    "                \"developer\": record.developer or \"\",\n",
    "                \"publisher\": record.publisher or \"\",\n",
    "                \"release_date\": record.release_date or \"\",\n",
    "                \"platforms\": \", \".join(record.platforms) if record.platforms else \"\",\n",
    "                \"source\": record.source or \"web\",\n",
    "                \"url\": record.url or \"\",\n",
    "            }\n",
    "            \n",
    "            # Add to vector store\n",
    "            vsm.add_document(doc_id=str(record.id), text=doc_text, metadata=metadata)\n",
    "            added += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to persist record {record.title}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return added\n",
    "''')\n",
    "    \n",
    "    print(\"âœ… Function added! Now restart the kernel and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d63bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… persist_new_knowledge function exists\n",
      "Function signature: (records)\n"
     ]
    }
   ],
   "source": [
    "# Check if persist_new_knowledge exists in workspace memory.py\n",
    "import memory\n",
    "import inspect\n",
    "\n",
    "if not hasattr(memory, 'persist_new_knowledge'):\n",
    "    print(\"âš ï¸ persist_new_knowledge function is missing from memory.py\")\n",
    "    print(f\"Memory module location: {memory.__file__}\")\n",
    "    print(\"\\nYou need to add this function to your workspace memory.py file.\")\n",
    "else:\n",
    "    print(\"âœ… persist_new_knowledge function exists\")\n",
    "    print(f\"Function signature: {inspect.signature(memory.persist_new_knowledge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89b510e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### UdaPlay Agent Responses ###\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ðŸŽ® UdaPlay Agent Response\n",
       "\n",
       "**Question:** When PokÃ©mon Gold and Silver was released?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: PokÃ©mon Gold and Silver released October 15, 2000 - 21 years ago ...\n",
       "\n",
       "PokÃ©mon Gold and Silver released October 15, 2000 - 21 years ago these games came out and I still love them.\n",
       "\n",
       "[Read more](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "\n",
       "### Source 2: PokÃ©mon Gold & Silver released on this day 25 years ago in Japan ...\n",
       "\n",
       "November 21, 1999. 25 years ago, PokÃ©mon Gold and Silver versions were released in Japan. While they did not release in the U.S. until nearly a\n",
       "\n",
       "[Read more](https://www.facebook.com/retrododo/posts/pok%C3%A9mon-gold-silver-released-on-this-day-25-years-ago-in-japan-it-was-a-long-11-/1122756426516311/)\n",
       "\n",
       "### Source 3: Prerelease:PokÃ©mon Gold and Silver - The Cutting Room Floor\n",
       "\n",
       "* November 15 - The demos for *PokÃ©mon Gold and Silver* at Space World '97 are compiled. * November 21 - *PokÃ©mon Gold and Silver* are released in Japan. Given how Dolly's existence was revealed to the world on 22 February 1997, when *Gold and Silver* had already been worked on for a year, it's then possible they originally intended for this PokÃ©mon to be included in those games. Additionally, in a November 2009 issue of Nintendo DREAM, game designer Morimoto Shigeki gave more details regarding Lugia's significance in *Gold and Silver*. In this interview, he namely said that while Ho-Oh and Lugia do not have a direct connection in the story, they were envisioned as being \"[...] born in the world of PokÃ©mon Gold and Silver for its new feature, the time system, giving us the day and night cycle. ## Region Map. The team originally had troubles figuring out what the world of *PokÃ©mon Gold and Silver* would look like.\n",
       "\n",
       "[Read more](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## ðŸ“š Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [PokÃ©mon Gold and Silver released October 15, 2000 - 21 years ago ...](https://www.reddit.com/r/Gameboy/comments/q8rmer/pok%C3%A9mon_gold_and_silver_released_october_15_2000/)\n",
       "2. [PokÃ©mon Gold & Silver released on this day 25 years ago in Japan ...](https://www.facebook.com/retrododo/posts/pok%C3%A9mon-gold-silver-released-on-this-day-25-years-ago-in-japan-it-was-a-long-11-/1122756426516311/)\n",
       "3. [Prerelease:PokÃ©mon Gold and Silver - The Cutting Room Floor](https://tcrf.net/Prerelease:Pok%C3%A9mon_Gold_and_Silver)\n",
       "\n",
       "## ðŸ” Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# ðŸŽ® UdaPlay Agent Response\n",
       "\n",
       "**Question:** Which one was the first 3D platformer Mario game?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: 3D Platform Games - Who Made Them First? - SPINE ONLINE\n",
       "\n",
       "# 3D Platform Games - Who Made Them First? What was the first 3D platformer? In the 80s, platformers with primitive 3D gameplay actually did exist; games like *3-D WorldRunner* attempted to provide a 3D experience on hardware that could only render 2D imagery. But if youâ€™d consider games like this to be 3D games, then 3D platformers got their start back in the 80s! But despite being the first game to feature many of the genreâ€™s now-staple mechanics, *Super Mario 64* wasnâ€™t the first 3D platformer. So while *Alpha Waves* didnâ€™t have the dramatic, long-term impact on the 3D platformer genre that some later games did, it *was* the very first! One of *Alpha Waves*â€™s most obvious differences compared to modern 3D platformers is in the controls. Itâ€™s definitely primitive, even compared to the 3D platformers that came just a few years later, but this control scheme makes *Alpha Waves* a unique experience, even today!\n",
       "\n",
       "[Read more](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "\n",
       "### Source 2: Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz\n",
       "\n",
       "## Forums - Gaming - Mario 64 wasnt the first true 3D platformer...Sony did it first. Mario 64 was the first \"RENDERED 3D\" game =). But Mario 64 WAS the first 3D platformer to actually BE fully 3D in it's gameplay, and to actually WORK, and play WELL, and be, you know....a fun game to play. This may have beaten Mario to market but while it has 3D graphics, and is a platform game it was never really a 3D platformer. There were quite a few 3d platformers before Mario 64. Mario 64 wasn't praised for being the first 3D platformer. But while Sega and Sony had their own attempts during the same time period at 3D rivals, those being Nights and Crash Bandicoot, both of which were excellent games for their time, Mario 64 was, again, the only one that was TRULY 3D, and it was just doing things that were so far advanced that nothing else could really keep up for many years.\n",
       "\n",
       "[Read more](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "\n",
       "### Source 3: Super Mario 64 - It Wasn't First 3D Platformer, But It Is ... - YouTube\n",
       "\n",
       "Super Mario 64 - It Wasn't First 3D Platformer, But It Is The First 3D.. | Retrospective\n",
       "JayBlastNetwork\n",
       "668 subscribers\n",
       "161 likes\n",
       "4630 views\n",
       "15 May 2024\n",
       "I don't like writing out boring descriptions, but I do love Mario 64. Though I question I always had was what was it's impact on 3D gaming? Some say it was the start, but theres a little more to it than that. What impact did Mario 64 have on 3D gaming? Find out in this episode of the JayBlastNetwork\n",
       "\n",
       "0:00 Intro/History\n",
       "6:16 Jumping Into 3D\n",
       "8:56 Early Castle Stages\n",
       "15:27 Powered Up to Power Down\n",
       "24:27 Mission Madness Of The Upper Floor\n",
       "32:18 Foolishness Atop The Final Floor\n",
       "42:41 Was 64 DS Better?\n",
       "1:00:18 Impact\n",
       "54 comments\n",
       "\n",
       "\n",
       "[Read more](https://www.youtube.com/watch?v=HDOBH0e-8F4)\n",
       "\n",
       "## ðŸ“š Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [3D Platform Games - Who Made Them First? - SPINE ONLINE](http://spineonline.co/video-game-history/2023/9/24/e073zy5ryzvb5aafw8as8s0nnq6pab-pf2by)\n",
       "2. [Mario 64 wasnt the first true 3D platformer...Sony did it first - VGChartz](https://gamrconnect.vgchartz.com/thread/167485/mario-64-wasnt-the-first-true-3d-platformersony-did-it-first/2/)\n",
       "3. [Super Mario 64 - It Wasn't First 3D Platformer, But It Is ... - YouTube](https://www.youtube.com/watch?v=HDOBH0e-8F4)\n",
       "\n",
       "## ðŸ” Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# ðŸŽ® UdaPlay Agent Response\n",
       "\n",
       "**Question:** Was Mortal Kombat X released for Playstation 5?\n",
       "\n",
       "**Confidence:** Low (used web search)\n",
       "\n",
       "## Answer\n",
       "\n",
       "### Source 1: Mortal Kombat X - Wikipedia\n",
       "\n",
       "***Mortal Kombat X*** is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. An upgraded version of *Mortal Kombat X*, titled ***Mortal Kombat XL***, was released on March 1, 2016, for PlayStation 4 and Xbox One, including all downloadable content characters from the two released Kombat Packs, almost all bonus alternate costumes available at the time of release, improved gameplay, and improved netcode. By July 2015, due to heavy criticism for the porting issues that plagued the PC release of the game, almost all references to *Mortal Kombat X* had been removed from High Voltage Software's Facebook page. On March 2, 2015, NetherRealm Studios announced that their mobile division would release an iOS/Android \"Android (operating system)\") version of *Mortal Kombat X* in April 2015. With the 1.11 update version of the mobile game released on December 6, 2016, Freddy Krueger who appeared as a DLC character in *MK9 \"Mortal Kombat (2011 video game)\")* was added as a mobile-exclusive character using his signature moves and X-Ray attack from MK9.\n",
       "\n",
       "[Read more](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "\n",
       "### Source 2: Mortal Kombat X - PS5 Gameplay - YouTube\n",
       "\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "Section Plays\n",
       "192000 subscribers\n",
       "91 likes\n",
       "10279 views\n",
       "4 Jun 2025\n",
       "Mortal Kombat X - PS5 Gameplay\n",
       "\n",
       "About this game :\n",
       "Mortal Kombat X is a 2015 fighting game developed by NetherRealm Studios and published by Warner Bros. Interactive Entertainment for Microsoft Windows, PlayStation 4, and Xbox One. It is the tenth main installment in the Mortal Kombat series and a sequel to Mortal Kombat (2011), taking place 25 years later after the events of its predecessor. High Voltage Software developed the Windows version of the game, with Polish studio QLOC taking over the work on it shortly after the release of Kombat Pack 1.\n",
       "\n",
       "Whoâ€™s Next? Experience the Next Generation of the 1 Fighting Franchise.\n",
       "\n",
       "Mortal Kombat X combines unparalleled, cinematic presentation with all new gameplay. For the first time, players can choose from multiple variations of each character impacting both strategy and fighting style.\n",
       "\n",
       "#ps5gameplay\n",
       "8 comments\n",
       "\n",
       "\n",
       "[Read more](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "\n",
       "### Source 3: Is Mortal Kombat X online still supported on PS5? - Facebook\n",
       "\n",
       "MKX no longer coming to Xbox 360 or PS3!â€‹â€‹ Waiting For Mortal Kombat X On Last Gen? Bad News.\n",
       "\n",
       "[Read more](https://www.facebook.com/groups/374725147397441/posts/1245046263698654/)\n",
       "\n",
       "## ðŸ“š Sources\n",
       "\n",
       "### Web Search Results\n",
       "\n",
       "1. [Mortal Kombat X - Wikipedia](https://en.wikipedia.org/wiki/Mortal_Kombat_X)\n",
       "2. [Mortal Kombat X - PS5 Gameplay - YouTube](https://www.youtube.com/watch?v=tqsw711ZuAk)\n",
       "3. [Is Mortal Kombat X online still supported on PS5? - Facebook](https://www.facebook.com/groups/374725147397441/posts/1245046263698654/)\n",
       "\n",
       "## ðŸ” Agent Reasoning\n",
       "\n",
       "1. Retrieved 5 documents from vector DB\n",
       "2. Evaluation: LLM evaluation failed: Responses.create() got an unexpected keyword argument 'response_format'\n",
       "3. Used web search, found 3 results\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: Invoke your agent\n",
    "# - When PokÃ©mon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X released for Playstation 5?\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Instantiate the agent (defined in previous cell)\n",
    "agent = UdaPlayAgent()\n",
    "\n",
    "# Questions to ask\n",
    "questions = [\n",
    "    \"When PokÃ©mon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "]\n",
    "\n",
    "print(\"### UdaPlay Agent Responses ###\\n\")\n",
    "for q in questions:\n",
    "    result = agent.run(q)\n",
    "    display(Markdown(result[\"markdown\"]))\n",
    "    \n",
    "     # TODO: Invoke your agent\n",
    "# # - When PokÃ©mon Gold and Silver was released?\n",
    "# # - Which one was the first 3D platformer Mario game?\n",
    "# # - Was Mortal Kombat X realeased for Playstation 5?\n",
    "\n",
    "# # TODO: Invoke your agent\n",
    "# # - When PokÃ©mon Gold and Silver was released?\n",
    "# # - Which one was the first 3D platformer Mario game?\n",
    "# # - Was Mortal Kombat X released for Playstation 5?\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "# from agent import UdaPlayAgent  # Ensure agent.py contains the UdaPlayAgent class\n",
    "\n",
    "# # Instantiate the agent\n",
    "# agent = UdaPlayAgent()\n",
    "\n",
    "# # Questions to ask\n",
    "# questions = [\n",
    "#     \"When PokÃ©mon Gold and Silver was released?\",\n",
    "#     \"Which one was the first 3D platformer Mario game?\",\n",
    "#     \"Was Mortal Kombat X released for Playstation 5?\"\n",
    "# ]\n",
    "\n",
    "# print(\"### UdaPlay Agent Responses ###\\n\")\n",
    "# for q in questions:\n",
    "#     result = agent.run(q)  # agent.run returns {\"markdown\": render(report)}\n",
    "#     display(Markdown(result[\"markdown\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
